{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Mike030668/Spliter_Text2Movi_Kandinskiy_22/blob/main/notebooks/Constract_steps_cickle_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2dCFBzIDVjLQ","executionInfo":{"status":"ok","timestamp":1710594587593,"user_tz":-180,"elapsed":667,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["!git clone https://github.com/Mike030668/Spliter_Text2Movi_Kandinskiy_22.git -q"]},{"cell_type":"markdown","metadata":{"id":"TJNOkxoJJOG_"},"source":["# Construct method to find points of start and end fragments movi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4273,"status":"ok","timestamp":1710594591861,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"WRSn86u98HGl","outputId":"e53e3d13-e1af-49ef-fd3c-f0fb08417f46"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import numpy as np\n","import torch\n","from torch import nn\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","MAIN_DIR =  \"/content/Spliter_Text2Movi_Kandinskiy_22\"\n","import sys\n","sys.path.append(MAIN_DIR)\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"zB0Kzp-DakPO"},"source":["## init rand data"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"JQ8htmqkWz0c","executionInfo":{"status":"ok","timestamp":1710594591862,"user_tz":-180,"elapsed":9,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["from utills.step_points import ComputeDiffPoints, Shuff_Reshuff"]},{"cell_type":"code","execution_count":226,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1710605460478,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"_3aRPe-3Y5Ay","outputId":"8364c812-5b25-40b7-bb25-2a4b07892157"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0 68 60 86 50  4 99 55 63 10 42 24 20 88 66 99]\n","Time labels [ 0 68 60 86 50  4 99 55 63 10 42 24 20 88 66 99]\n","\n","\n","Norm_way\n","Start base point 0\n","Base normal points [68 60 86 50  4 99 55 63 10 42 24 20 88 66 99]\n","\n","\n","\n","Back_way\n","Start base back_point 0\n","Base back points [33 11 79 75 57 89 36 44  0 95 49 13 39 31 99]\n","\n","\n"]}],"source":["cdp = ComputeDiffPoints()\n","\n","# make set rand embeddings for tests\n","d_batch = 15\n","MAX_LEN_MOVI = 100\n","\n","# sets images embeds\n","image_embeds = torch.randn(d_batch+1, 1, 1280)\n","\n","# text embedings prompt video from Text model\n","text_hid_state = torch.randn((1, 77, 1280), requires_grad=True)\n","\n","# unclip embedings prompt video from Prior\n","unclip_embed = torch.randn((1, 1, 1280), requires_grad=True)\n","\n","# make set time_labels\n","labels = np.random.choice(np.arange(1, MAX_LEN_MOVI), d_batch+1, replace=False)\n","\n","# start and end points to use for start normal and back ways\n","labels[0] = 0\n","labels[-1] = MAX_LEN_MOVI-1\n","\n","print(labels)\n","cdp.time_labels = labels\n","c\n","\n","print(f\"Time labels {labels}\\n\")\n","print('\\nNorm_way')\n","print(f\"Start base point {cdp.s_point}\")\n","print(f\"Base normal points {cdp.points}\\n\\n\")\n","print('\\nBack_way')\n","print(f\"Start base back_point {cdp.back_s_point}\")\n","print(f\"Base back points {cdp.back_points}\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"-Oq9QFVcaqLb"},"source":["## compute normal and back points for rotation steps"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1710236373400,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"_Qp3aR2hurQB","outputId":"0e916a73-6d96-4015-97c5-c9d4c77446d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time labels [ 0 14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","\n","\n","Norm rotation way\n","Base normal points [14 20  5 35 82 97 80 13 24 42 23  3 63 90 99] for start rotation steps \n","\n","ID image and text start rotation points [2, 7, 11, 0, 2, 7, 11, 11, 0, 1, 2, 7, 8, 10, 11, 0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 2, 11, 0, 1, 2, 7, 10, 11, 0, 1, 2, 3, 7, 8, 10, 11, 0, 1, 2, 7, 11, 0, 1, 2, 3, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","ID uclip_emb start rotation points [2, 7, 11, 0, 2, 7, 11, 11, 0, 1, 2, 7, 8, 10, 11, 0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 2, 11, 0, 1, 2, 7, 10, 11, 0, 1, 2, 3, 7, 8, 10, 11, 0, 1, 2, 7, 11, 0, 1, 2, 3, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","ID delta rotation points [0, 0, 0, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n","Delta between points [9, 1, 11, 6, 15, 7, 17, 2, 21, 15, 30, 22, 11, 12, 32, 68, 62, 77, 47, 2, 69, 58, 40, 59, 79, 19, 83, 77, 92, 62, 15, 17, 84, 73, 55, 74, 94, 34, 7, 66, 60, 75, 45, 67, 56, 38, 57, 77, 17, 8, 10, 10, 4, 19, 11, 1, 21, 28, 22, 37, 7, 29, 18, 19, 39, 9, 3, 18, 10, 20, 49, 43, 58, 28, 50, 39, 21, 40, 60, 76, 70, 85, 55, 8, 10, 77, 66, 48, 67, 87, 27, 85, 79, 94, 64, 17, 2, 19, 86, 75, 57, 76, 96, 36, 9] normal points\n","\n","\n","Norm rotation way\n","Base back points [ 9 36 96 76 57 75 86 19  2 17 64 94 79 85 99] for start rotation back steps \n","\n","ID image and text start rotation points [8, 0, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 4, 5, 7, 8, 9, 10, 0, 1, 7, 8, 9, 0, 1, 4, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 13, 0, 8, 9, 0, 8, 0, 1, 4, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 0, 1, 3, 4, 5, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] in base back points\n","ID uclip_emb start rotation points [8, 0, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 4, 5, 7, 8, 9, 10, 0, 1, 7, 8, 9, 0, 1, 4, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 13, 0, 8, 9, 0, 8, 0, 1, 4, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 0, 1, 3, 4, 5, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] in base back points\n","ID delta rotation points [0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14] in base back points\n","Delta between points [7, 27, 17, 34, 19, 87, 60, 20, 39, 21, 10, 77, 94, 79, 32, 2, 17, 11, 67, 40, 19, 1, 57, 74, 59, 12, 48, 21, 38, 55, 40, 66, 39, 18, 56, 73, 58, 11, 77, 50, 10, 29, 11, 67, 84, 69, 22, 7, 1, 10, 17, 2, 8, 15, 55, 28, 7, 45, 62, 47, 85, 58, 18, 37, 19, 8, 75, 92, 77, 30, 15, 9, 70, 43, 3, 22, 4, 60, 77, 62, 15, 76, 49, 9, 28, 10, 66, 83, 68, 21, 6, 90, 63, 3, 23, 42, 24, 13, 80, 97, 82, 35, 5, 20, 14] back points\n"]}],"source":["print(f\"Time labels {labels}\")\n","print('\\n\\nNorm rotation way')\n","print(f\"Base normal points {cdp.points} for start rotation steps \\n\")\n","id_img_emb_s = config_norm[\"id_img_emb_s\"]\n","print(f\"ID image and text start rotation points {id_img_emb_s}\")\n","id_uclip_emb = config_norm[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start rotation points {id_uclip_emb}\")\n","id_img_delta = config_norm[\"id_img_delta\"]\n","print(f\"ID delta rotation points {id_img_delta}\")\n","delta = config_norm[\"delta\"]\n","print(f\"Delta between points {delta} normal points\")\n","\n","\n","print('\\n\\nNorm rotation way')\n","print(f\"Base back points {cdp.back_points} for start rotation back steps \\n\")\n","id_img_emb_s = config_back[\"id_img_emb_s\"]\n","print(f\"ID image and text start rotation points {id_img_emb_s} in base back points\")\n","id_uclip_emb = config_back[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start rotation points {id_uclip_emb} in base back points\")\n","id_img_delta = config_back[\"id_img_delta\"]\n","print(f\"ID delta rotation points {id_img_delta} in base back points\")\n","delta = config_back[\"delta\"]\n","print(f\"Delta between points {delta} back points\")\n"]},{"cell_type":"markdown","metadata":{"id":"Tp7w3zw7TmDp"},"source":["## compute normal and back points for diff steps\n","\n","It will be used for prediction from prediction of model"]},{"cell_type":"code","execution_count":218,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1710605068133,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"D2Q7stqHZbbM","outputId":"aac4714c-0b19-404f-a13f-1d29a3534047"},"outputs":[{"output_type":"stream","name":"stdout","text":["Time labels [ 0 77 38 96 11 53 50 44  9 95 29 49 15  7 26 99]\n","\n","\n","Norm diff way\n","Base normal points [77 38 96 11 53 50 44  9 95 29 49 15  7 26 99]\n","Next normal points [ 78  39  97  12  54  51  45  10  96  30  50  16   8  27 100] for diff steps \n","\n","ID image and text start diff points [5] in base normal points\n","ID uclip_emb start diff points [10] in next normal points\n","ID delta diff points [2] in next normal points\n","Delta between diff points [46] in base normal points\n","\n","\n","Norm rotation way\n","Base back points [73 92 84 50 70  4 90 55 49 46 88  3 61 22 99]\n","Next back points [ 74  93  85  51  71   5  91  56  50  47  89   4  62  23 100] for diff steps \n","\n","ID image and text start diff points [5] in base back points\n","ID uclip_emb start diff points [11] in next back points\n","ID delta diff points [3] in next back points\n","Delta between diff points [46] back points\n"]}],"source":["config_diff_norm, config_diff_back = cdp.getpoints_diftrain()\n","\n","print(f\"Time labels {labels}\")\n","print('\\n\\nNorm diff way')\n","print(f\"Base normal points {cdp.points}\")\n","print(f\"Next normal points {cdp.next_points} for diff steps \\n\")\n","id_img_emb_s = config_diff_norm[\"id_img_emb_s\"]\n","print(f\"ID image and text start diff points {id_img_emb_s} in base normal points\")\n","id_uclip_emb = config_diff_norm[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start diff points {id_uclip_emb} in next normal points\")\n","id_img_delta = config_diff_norm[\"id_img_delta\"]\n","print(f\"ID delta diff points {id_img_delta} in next normal points\")\n","delta = config_diff_norm[\"delta\"]\n","print(f\"Delta between diff points {delta} in base normal points\")\n","\n","\n","print('\\n\\nNorm rotation way')\n","print(f\"Base back points {cdp.back_points}\")\n","print(f\"Next back points {cdp.back_next_points} for diff steps \\n\")\n","id_img_emb_s = config_diff_back[\"id_img_emb_s\"]\n","print(f\"ID image and text start diff points {id_img_emb_s} in base back points\")\n","id_uclip_emb = config_diff_back[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start diff points {id_uclip_emb} in next back points\")\n","id_img_delta = config_diff_back[\"id_img_delta\"]\n","print(f\"ID delta diff points {id_img_delta} in next back points\")\n","delta = config_diff_back[\"delta\"]\n","print(f\"Delta between diff points {delta} back points\")\n"]},{"cell_type":"markdown","metadata":{"id":"eRPXhwREGqQI"},"source":["# Both ways steps model"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"EH5Kf3UVY3gF","executionInfo":{"status":"ok","timestamp":1710594597525,"user_tz":-180,"elapsed":902,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["from models.R_baseline import R_SpliterSimple\n","from utills.clearing import flush_memory\n","\n","model = R_SpliterSimple(max_delta_time = 100,\n","                        emb_dim = 1280,\n","                        ways = 2\n","                        )"]},{"cell_type":"markdown","metadata":{"id":"fLGgqgDL778b"},"source":["## Losses\n","\n","nn.CosineEmbeddingLoss\n","https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1258,"status":"ok","timestamp":1710594606991,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"l05KihJir06z","outputId":"b12cc415-9943-4268-db8f-add13262d3b2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 1, 1280])"]},"metadata":{},"execution_count":5}],"source":["clip_text_last_hidden_state = torch.randn((2, 77, 1280)) #outputs.last_hidden_state\n","embeddings = torch.randn((2, 1, 1280))\n","delta = torch.tensor([[3],[5]])\n","direction  = torch.tensor([[0],[1]])\n","model.to(DEVICE)\n","out_embeddings = model(text_hidden_states = clip_text_last_hidden_state.to(DEVICE),\n","                       prior_embeds = embeddings.to(DEVICE),\n","                       delta_time = delta.to(DEVICE),\n","                       direction = direction.to(DEVICE)\n","                        )\n","out_embeddings.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1710594608377,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"YRthV5Nc5bHJ","outputId":"b5270809-729e-42d3-e6e5-ed5c04fa0daa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0415, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":6}],"source":["# https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\n","loss = nn.CosineEmbeddingLoss()\n","input1 = torch.randn(3, 1, requires_grad=True)\n","input2 = torch.randn(3, 5, requires_grad=True)\n","target = torch.ones(3)\n","output = loss(input1, input2, target)\n","output.backward()\n","output"]},{"cell_type":"markdown","metadata":{"id":"GVtldMSOUWx7"},"source":["## apply as vector losses"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1710594610806,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"2g-uuGIc6Zm_","outputId":"5f8a27f5-8e5b-4d1f-997a-7f80d9221d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.8024, 1.9816, 0.0150,  ..., 0.3603, 0.0881, 1.3852], device='cuda:0',\n","       grad_fn=<AddBackward0>)\n"]}],"source":["mse_loss = nn.MSELoss(reduction='none').to(DEVICE)\n","cos_loss = nn.CosineEmbeddingLoss(reduction = 'none').to(DEVICE)\n","target = torch.ones(1280).to(DEVICE)\n","loss_cos = cos_loss(embeddings.squeeze(1).T.to(DEVICE), out_embeddings.squeeze(1).T, target)\n","print(loss_cos)"]},{"cell_type":"markdown","metadata":{"id":"cL-QqPMYGqQI"},"source":["## first steps normal and back shuffleed"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1710594721786,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"mO_PB6bQGqQJ","outputId":"3c347e37-9620-479f-b2d2-27c7f5506277"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 2.1621, 10.9531,  7.4219,  ...,  2.7754,  1.9805,  2.3164],\n","       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)\n"]}],"source":["all_points = np.append(cdp.points, cdp.back_points)\n","# intit class for shufflee\n","b_srs = Shuff_Reshuff(len(all_points))\n","bs_idx = b_srs.shuffle() # shuffleed indexees\n","bu_idx = b_srs.unshuffle() # indexees for re_shuffle\n","\n","B_batch = 2*d_batch\n","\n","time_base = torch.tensor(all_points).unsqueeze(1)\n","directions = torch.concat([torch.zeros_like(time_base[:d_batch]), torch.ones_like(time_base[d_batch:])])\n","\n","# shuffleed\n","time_base = time_base.to(DEVICE)\n","directions = directions.to(DEVICE)\n","\n","\n","text_hid_states =  torch.concat([text_hid_state for _ in range(B_batch)])\n","base_unclip_embs = torch.concat([unclip_embed for _ in range(B_batch)])\n","\n","base_img_embs = [image_embeds[0] for _ in range(d_batch)] + [image_embeds[-1] for _ in range(d_batch)]\n","base_img_embs = torch.concat(base_img_embs)\n","\n","\n","back_img_embs = torch.concat([image_embeds[i,:,:] for i in range(image_embeds.shape[0])][::-1]).unsqueeze(1)\n","img_embs =[image_embeds[1:,:,:], back_img_embs[1:,:,:]]\n","img_embs = torch.concat(img_embs)\n","\n","\n","pred_unclip_embs = model(\n","      text_hidden_states = text_hid_states[bs_idx].to(torch.float32).to(DEVICE), # shuffleed\n","      prior_embeds = base_unclip_embs[bs_idx].to(torch.float32).to(DEVICE),\n","      delta_time = time_base[bs_idx],\n","      direction = directions[bs_idx]\n","                        )\n","\n","# difference\n","diff_img_embs =  (base_img_embs[bs_idx].squeeze(dim=1) - img_embs[bs_idx].squeeze(dim=1)) #\n","diff_unclip_embs = (base_unclip_embs[bs_idx].squeeze(dim=1).to(DEVICE) - pred_unclip_embs.squeeze(dim=1))\n","\n","# CosineEmbeddingLoss between difference\n","movi_cos_loss = cos_loss(diff_unclip_embs.T, diff_img_embs.T.to(DEVICE), target).half()  #\n","\n","# MSELoss between predict and\n","movi_mse_loss = mse_loss(diff_unclip_embs.half(), diff_img_embs.half().to(DEVICE))#\n","\n","movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","del(time_base, directions, diff_img_embs, diff_unclip_embs)\n","flush_memory()\n","\n","print(movi_loss)"]},{"cell_type":"markdown","metadata":{"id":"zWh3qG0Nseff"},"source":["## Rotation steps normal and back shuffleed"]},{"cell_type":"code","source":["class RotationVectors:\n","    def unit_vector(self, vectors):\n","        \"\"\"\n","        TORCH\n","        Returns the unit vectors of the input vectors.\n","        vectors: torch tensor of shape (batch_size, vector_dim)\n","        \"\"\"\n","        return vectors / torch.norm(vectors, dim=1, keepdim=True)\n","\n","    def angle(self, vectors1, vectors2):\n","        \"\"\"\n","        Returns the angles in radians between the given batches of vectors.\n","        vectors1, vectors2: torch tensors of shape (batch_size, vector_dim)\n","        \"\"\"\n","        v1_u = self.unit_vector(vectors1)\n","        v2_u = self.unit_vector(vectors2)\n","\n","        minor = torch.det(torch.stack((v1_u[:, -2:], v2_u[:, -2:]), dim=1))\n","        if (minor == 0).any():\n","            raise NotImplementedError('Some vectors are too odd!')\n","        return torch.sign(minor) * torch.acos(torch.clamp(torch.sum(v1_u * v2_u, dim=1), -1.0, 1.0))\n","\n","    def get_rotation_matrix(self, vec_1, vec_2):\n","        \"\"\"\n","        Compute rotation matrices between batches of vectors.\n","        vec_1, vec_2: torch tensors of shape (batch_size, vector_dim)\n","        \"\"\"\n","        a = self.angle(vec_1, vec_2)\n","        n1 = self.unit_vector(vec_1)\n","\n","        vec_2 = vec_2 - torch.sum(n1 * vec_2, dim=1, keepdim=True) * n1\n","        n2 = self.unit_vector(vec_2)\n","\n","        # Assuming vec_1, n1, n2, and a are already defined as torch tensors\n","        I = torch.eye(vec_1.shape[1]).to(vec_1.device)\n","        R = I + ((n2.unsqueeze(2) @ n1.unsqueeze(1)) - (n1.unsqueeze(2) @ n2.unsqueeze(1))) * torch.sin(a).unsqueeze(1).unsqueeze(2) + \\\n","            ((n1.unsqueeze(2) @ n1.unsqueeze(1)) + (n2.unsqueeze(2) @ n2.unsqueeze(1))) * (torch.cos(a).unsqueeze(1).unsqueeze(2) - 1)\n","\n","        return R\n","\n","    def cosim_rotate(self, b_matrix, cos_sim_matrix, b_R_matrix):\n","         \"\"\"\n","         Rotate batch_vectors as matrix with batch R_matrixes\n","         with using batch cos_sim these batch_vectors to engine vectors\n","         \"\"\"\n","         return torch.mul((1 - cos_sim_matrix).unsqueeze(1), b_matrix) + torch.mul(cos_sim_matrix.unsqueeze(1), b_matrix @ b_R_matrix)"],"metadata":{"id":"yvvk0gqjYVVJ","executionInfo":{"status":"ok","timestamp":1710605801962,"user_tz":-180,"elapsed":763,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"execution_count":232,"outputs":[]},{"cell_type":"code","execution_count":210,"metadata":{"id":"IF4rY1xAwInw","executionInfo":{"status":"ok","timestamp":1710604304492,"user_tz":-180,"elapsed":273,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["#from utills.rotations import RotationVectors\n","# inite rotation class\n","RV = RotationVectors()"]},{"cell_type":"code","execution_count":215,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":759,"status":"ok","timestamp":1710605006355,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"3uY7utEtutNJ","outputId":"53f33a74-52ef-4e2a-b5ac-652bd803b03b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([15.8672, 36.0000, 31.0781,  ..., 18.8438, 16.2969, 14.0156],\n","       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)\n"]}],"source":["rote_norm_train = len(config_norm['id_uclip_emb'])\n","rote_back_train = len(config_back['id_uclip_emb'])\n","rote_train =  rote_norm_train + rote_back_train\n","\n","if rote_train:\n","    text_hid_states_2rt = []\n","    unclip_embs_2rt = []\n","    base_img_embs_2rt = []\n","    image_embs_2rt = []\n","    delta_2rt = []\n","    direction_2rt = []\n","\n","    # intit class for shufflee again\n","    srs = Shuff_Reshuff(rote_train)\n","    s_idx = srs.shuffle() # shuffleed indexees\n","    u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","\n","    if rote_norm_train:\n","        # collect norm steps\n","        text_hid_states_2rt.append(torch.clone(text_hid_states[:d_batch])[config_norm['id_uclip_emb']])\n","        unclip_embs_2rt.append(torch.clone(base_unclip_embs[:d_batch])[config_norm['id_uclip_emb']])\n","        base_img_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_emb_s']])\n","        image_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_delta']])\n","        delta_2rt.append(torch.tensor(config_norm['delta']).unsqueeze(1))\n","        direction_2rt.append(torch.zeros_like(delta_2rt[-1]))\n","\n","\n","    if rote_back_train:\n","        # collect back steps\n","        text_hid_states_2rt.append(torch.clone(text_hid_states[d_batch:])[config_back['id_uclip_emb']])\n","        unclip_embs_2rt.append(torch.clone(base_unclip_embs[d_batch:])[config_back['id_uclip_emb']])\n","        base_img_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_emb_s']])\n","        image_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_delta']])\n","        delta_2rt.append(torch.tensor(config_back['delta']).unsqueeze(1))\n","        direction_2rt.append(torch.ones_like(delta_2rt[-1]))\n","\n","    # concat tansors\n","    if rote_train > B_batch:\n","       R_batch = B_batch\n","    else: R_batch = rote_train\n","\n","    # shufle and take R_batch samples\n","    text_hid_states_2rt = torch.concat(text_hid_states_2rt)[s_idx][:R_batch].to(DEVICE)\n","    unclip_embs_2rt = torch.concat(unclip_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","    base_img_embs_2rt = torch.concat(base_img_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","    image_embs_2rt = torch.concat(image_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","    delta_2rt = torch.concat(delta_2rt)[s_idx][s_idx][:R_batch].to(DEVICE)\n","    direction_2rt = torch.concat(direction_2rt)[s_idx][:R_batch].to(DEVICE)\n","\n","    # get cos_sim  base_img_embs vectors and base_unclip_embs vectors\n","    cos_sim = torch.cosine_similarity(base_img_embs.unsqueeze(1), base_unclip_embs, dim = 2)\n","\n","    # get rotation marixes_i2i\n","    R_marixes_i2i = RV.get_rotation_matrix(base_img_embs.to(DEVICE), base_img_embs_2rt.squeeze(1)).to(DEVICE)\n","\n","    # compute roted unclip_embs with R_marixes_i2i and cos_sim base_img_embs and base_unclip_embs\n","    unclip_embs_2rt = RV.cosim_rotate(unclip_embs_2rt, cos_sim.to(DEVICE), R_marixes_i2i)\n","\n","    # get rotation marixes_u2u\n","    R_marixes_u2u = RV.get_rotation_matrix(base_unclip_embs.squeeze(1).to(torch.float32).to(DEVICE),\n","                                                           unclip_embs_2rt.squeeze(1))\n","\n","    # compute roted text_hid_states with R_marixes_u2u and cos_sim base_img_embs and base_unclip_embs\n","    text_hid_states_2rt = text_hid_states_2rt @ R_marixes_u2u\n","\n","    pred_rote_embs = model(\n","                  text_hidden_states = text_hid_states_2rt.to(torch.float32), # shuffleed\n","                  prior_embeds = unclip_embs_2rt.to(torch.float32),\n","                  delta_time = delta_2rt,\n","                  direction = direction_2rt\n","                                    )\n","\n","    # difference\n","    diff_unclip_embeds = (unclip_embs_2rt.squeeze(dim=1) - pred_rote_embs.squeeze(dim=1))\n","    diff_img_embs =  (base_img_embs_2rt.squeeze(dim=1) - image_embs_2rt.squeeze(dim=1))\n","\n","    # CosineEmbeddingLoss between difference\n","    movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n","\n","    # MSELoss between predict and\n","    movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n","\n","    movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","    del(delta_2rt, direction_2rt, diff_img_embs, diff_unclip_embeds)\n","    del(pred_rote_embs, text_hid_states_2rt, R_marixes_i2i, R_marixes_u2u)\n","    del(unclip_embs_2rt,image_embs_2rt)\n","    flush_memory()\n","\n","\n","    print(movi_loss)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FEz3tuuKBMHD"},"source":["## Next steps normal and back from pred steps shuffleed"]},{"cell_type":"code","execution_count":219,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1710605079278,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"zQg68d4NGqQJ","outputId":"9cd3d72c-9a1f-4cfb-c4e4-093f2465b81c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.7217, 2.8125, 2.3320,  ..., 3.9258, 2.1113, 0.9697], device='cuda:0',\n","       dtype=torch.float16, grad_fn=<AddBackward0>)\n"]}],"source":["diff_norm = len(config_diff_norm['id_uclip_emb'])\n","diff_back = len(config_diff_back['id_uclip_emb'])\n","diff_train = diff_norm + diff_back\n","\n","if diff_train:\n","    take_text_hid_states = []\n","    take_base_unclip_embs = []\n","    next_unclip_embs = []\n","    next_base_img_embs = []\n","    next_image_embs = []\n","    next_delta = []\n","    next_text_hid_states = []\n","    next_direction = []\n","\n","    # un_shuffleed\n","    pred_unclip_embs = torch.clone(pred_unclip_embs.detach().cpu())[bu_idx]\n","\n","    # intit class for shufflee again\n","    srs = Shuff_Reshuff(diff_train)\n","    s_idx = srs.shuffle() # shuffleed indexees\n","    u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","    if diff_norm:\n","        # collect next norm steps\n","        take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","        take_text_hid_states.append(torch.clone(text_hid_states[:d_batch])[config_diff_norm['id_uclip_emb']])\n","        next_unclip_embs.append(torch.clone(pred_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","        next_base_img_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_emb_s']])\n","        next_image_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_delta']])\n","        next_delta.append(torch.tensor(config_diff_norm['delta']).unsqueeze(1))\n","        next_direction.append(torch.zeros_like(next_delta[-1]))\n","\n","    if diff_back:\n","        # collect next back steps\n","        take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_back['id_uclip_emb']])\n","        take_text_hid_states.append(torch.clone(text_hid_states[d_batch:])[config_diff_back['id_uclip_emb']])\n","        next_unclip_embs.append(torch.clone(pred_unclip_embs[d_batch:])[config_diff_back['id_uclip_emb']])\n","        next_base_img_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_emb_s']])\n","        next_image_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_delta']])\n","        next_delta.append(torch.tensor(config_diff_back['delta']).unsqueeze(1))\n","        next_direction.append(torch.ones_like(next_delta[-1]))\n","\n","\n","\n","    take_base_unclip_embs = torch.concat(take_base_unclip_embs).to(DEVICE)\n","    take_text_hid_states = torch.concat(take_text_hid_states).to(DEVICE)\n","    next_unclip_embs = torch.concat(next_unclip_embs).to(DEVICE)\n","    next_base_img_embs = torch.concat(next_base_img_embs)\n","    next_image_embs = torch.concat(next_image_embs)\n","    next_delta = torch.concat(next_delta)[s_idx].to(DEVICE)\n","    next_direction = torch.concat(next_direction)[s_idx].to(DEVICE)\n","\n","    # get rotation vectors\n","    R_marixes = RV.get_rotation_matrix(take_base_unclip_embs.squeeze(1), next_unclip_embs.squeeze(1))\n","    next_text_hid_states = take_text_hid_states @ R_marixes\n","\n","\n","    next_pred_unclip_embs = model(\n","        text_hidden_states = next_text_hid_states[s_idx].to(torch.float32),\n","        prior_embeds = next_unclip_embs[s_idx].to(torch.float32),\n","        delta_time = next_delta,\n","        direction = next_direction\n","        )\n","\n","    # difference\n","    diff_unclip_embeds = (next_unclip_embs[s_idx].squeeze(dim=1).to(DEVICE) - next_pred_unclip_embs.squeeze(dim=1))\n","    diff_img_embs =  (next_base_img_embs[s_idx].squeeze(dim=1) - next_image_embs[s_idx].squeeze(dim=1))\n","\n","    # CosineEmbeddingLoss between difference\n","    movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n","\n","    # MSELoss between predict and\n","    movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n","\n","    next_movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","    del(diff_img_embs, diff_unclip_embeds)\n","    del(next_image_embs, next_base_img_embs, next_delta, next_direction)\n","    del(next_text_hid_states,  next_unclip_embs)\n","    del(pred_unclip_embs, next_pred_unclip_embs)\n","    flush_memory()\n","\n","    print(next_movi_loss)"]},{"cell_type":"markdown","metadata":{"id":"XW6o9w1mOQfS"},"source":["##  Cicle"]},{"cell_type":"markdown","metadata":{"id":"u2c2tlUzNLRe"},"source":["### init rand data"]},{"cell_type":"code","execution_count":233,"metadata":{"id":"j8ekLAdPOQfT","executionInfo":{"status":"ok","timestamp":1710605816488,"user_tz":-180,"elapsed":590,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["all_data = []\n","# make set rand embeddings for tests\n","d_batch = 15\n","q_ty = 100\n","\n","for i in range(q_ty):\n","    data = dict()\n","    # make set time_labels\n","    labels = np.random.choice(np.arange(1, 99), d_batch+1, replace=False)\n","    labels[0] = 0\n","    labels[-1] = 99\n","    data['labels'] = labels\n","\n","    data['image_embeds'] = []\n","    for i in range(d_batch+1):\n","      if not i : data['image_embeds'].append(torch.randn(1, 1, 1280))\n","      else:\n","        next = data['image_embeds'][-1] + 0.0001*torch.randn(1, 1, 1280)\n","        data['image_embeds'].append(next)\n","\n","    data['text_hid_state'] = torch.randn((1, 77, 1280))\n","    data['unclip_embed'] = torch.randn((1, 1, 1280), requires_grad=True)\n","\n","    all_data.append(data)\n"]},{"cell_type":"code","execution_count":234,"metadata":{"id":"5MrmU4BvOQfT","executionInfo":{"status":"ok","timestamp":1710605818569,"user_tz":-180,"elapsed":5,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["#from utills.rotations import RotationVectors\n","# inite rotation class\n","LR_RATE = 1e-03\n","EPS = 0.95\n","\n","mse_loss = nn.MSELoss(reduction='none').to(DEVICE)\n","cos_loss = nn.CosineEmbeddingLoss(reduction = 'none').to(DEVICE)\n","target = torch.ones(1280).to(DEVICE)\n","\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR_RATE)\n","\n","# https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n","scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n","                                              cycle_momentum = False,\n","                                              base_lr=LR_RATE/10,\n","                                              max_lr=LR_RATE,\n","                                              step_size_up=5,\n","                                              mode=\"triangular2\")\n","\n","add_back_train = True\n","add_rote_train = True\n","add_diff_train = True\n","\n","model.to(DEVICE)\n","model.train()\n","\n","# inite rotation class\n","RV = RotationVectors()\n","\n","# inite ComputeDiffPoints class\n","cdp = ComputeDiffPoints(treshold = 0)"]},{"cell_type":"markdown","metadata":{"id":"ZWAmLlguOEtl"},"source":["### test train"]},{"cell_type":"code","execution_count":237,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["8696922233694517b2a451c2309245d7","9eecd69d34394dc78d894c55ce063287","2ad8acf31f614c52b85efe087dde4c83","a9d63dd51dae4eb8b6e28a2512f014f7","7d6325a736a34303ba71aecfcee00ac7","7d3515d9ada34ef1bbd3be08eccbae90","8da9a3bccb354809a2843a245b98c9ba","cc4d1b3c124f4558a8cf2012fb253e14","3cc581f3f73c41fa80b65b5454b1f224","6b76180007f846808c6754a80801b855","4e134169a93b4d1289a7b5182bafe4d8"]},"executionInfo":{"elapsed":52937,"status":"ok","timestamp":1710605975712,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"OU35XBiQ8Z69","outputId":"533ce1af-4649-419a-dda3-ef6b50b04c59"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Пробегаемся по всем эпохам:   0%|          | 0/3 [00:00<?, ?EPOHS/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8696922233694517b2a451c2309245d7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["diff_loss_weight 0\n","\rEp 0 all_loss 0.02125 | acc 0.39280 | mse_loss 0.01113 | cos_loss 0.01012 | lr 0.00027\n","\n","diff_loss_weight 0\n","Ep 1 all_loss 0.02891 | acc 0.38917 | mse_loss 0.01481 | cos_loss 0.01410 | lr 0.00026\n","\n","diff_loss_weight 0.022977941176470562\n","Ep 2 all_loss 0.02996 | acc 0.38574 | mse_loss 0.01578 | cos_loss 0.01418 | lr 0.00026\n","\n"]}],"source":["# отрисовка прохождения цикла\n","from tqdm.notebook import tqdm\n","from random import shuffle\n","import time\n","\n","step = 0\n","EPOHS = 3\n","\n","hist = {\"loss\" : [],\n","        \"loss_cos\" : [],\n","        \"loss_mse\" : [],\n","        \"acc\" : [],\n","        \"base_loss\" : [],\n","        \"lr\" : []\n","}\n","\n","for epoch  in tqdm(range(EPOHS), unit =\"EPOHS\",\n","                      desc =\"Пробегаемся по всем эпохам\"):\n","    eph_loss = 0\n","    eph_loss_cos = 0\n","    eph_loss_mse = 0\n","    eph_cos_acc = 0\n","    eph_base_loss = 0\n","    cur_lr = optimizer.param_groups[0]['lr']\n","    diff_loss_weight = 0\n","\n","    shuffle(all_data)\n","    for movi in all_data:\n","\n","        optimizer.zero_grad()\n","        # make set rand embeddings for tests\n","        image_embeds = torch.concat(movi['image_embeds'])\n","        text_hid_state = movi['text_hid_state']\n","        unclip_embed = movi['unclip_embed']\n","        # make set time_labels\n","        labels = movi['labels']\n","\n","        # place labels to class points\n","        cdp.time_labels = labels\n","        config_norm, config_back = cdp.getpoints_train()\n","        all_points = cdp.points\n","\n","        if add_back_train:\n","            all_points = np.append(all_points, cdp.back_points)\n","\n","        # intit class for shufflee\n","        b_srs = Shuff_Reshuff(len(all_points))\n","        bs_idx = b_srs.shuffle() # shuffleed indexees\n","        bu_idx = b_srs.unshuffle() # indexees for re_shuffle\n","\n","        # collect time directions tensors\n","        time_base = torch.tensor(all_points).unsqueeze(1)\n","\n","        directions = [torch.zeros_like(time_base[:d_batch])]\n","        if add_back_train:\n","            directions.append(torch.ones_like(time_base[d_batch:]))\n","        directions = torch.concat(directions)\n","\n","\n","        # collect text tensors\n","        text_hid_states = [text_hid_state for _ in range(d_batch)]\n","        if add_back_train:\n","            text_hid_states.extend([text_hid_state for _ in range(d_batch)])\n","        text_hid_states =  torch.concat(text_hid_states)\n","\n","        # collect unclip tensors\n","        base_unclip_embs = [unclip_embed for _ in range(d_batch)]\n","        if add_back_train:\n","            base_unclip_embs.extend([unclip_embed for _ in range(d_batch)])\n","        base_unclip_embs = torch.concat(base_unclip_embs)\n","\n","        # collect base_img_embs tensors\n","        base_img_embs = [image_embeds[0] for _ in range(d_batch)]\n","        if add_back_train:\n","            base_img_embs.extend([image_embeds[-1] for _ in range(d_batch)])\n","        base_img_embs = torch.concat(base_img_embs)\n","\n","        # collect img_embs tensors\n","        img_embs = image_embeds[1:]\n","\n","        if add_back_train:\n","            # collect back_img_embs tensors\n","            back_img_embs = torch.flip(image_embeds, [0,])[1:]\n","            # collect img_embs together tensors\n","            img_embs = torch.concat([img_embs, back_img_embs])\n","\n","        # base predict which can used in next step\n","        pred_unclip_embs = model(\n","              text_hidden_states = text_hid_states[bs_idx].to(torch.float32).to(DEVICE), # shuffleed\n","              prior_embeds = base_unclip_embs[bs_idx].to(torch.float32).to(DEVICE),\n","              delta_time = time_base[bs_idx].to(DEVICE),\n","              direction = directions[bs_idx].to(DEVICE)\n","                                )\n","\n","        # difference\n","        diff_img_embs = (base_img_embs[bs_idx].squeeze(dim=1) - img_embs[bs_idx].squeeze(dim=1)) #\n","        diff_unclip_embs = (base_unclip_embs[bs_idx].squeeze(dim=1).to(DEVICE) - pred_unclip_embs.squeeze(dim=1))\n","\n","        # CosineEmbeddingLoss between difference\n","        movi_cos_loss = cos_loss(diff_unclip_embs.T, diff_img_embs.T.to(DEVICE), target).half()  #\n","\n","        # MSELoss between predict and\n","        movi_mse_loss = mse_loss(diff_unclip_embs.half(), diff_img_embs.half().to(DEVICE)).mean(0)#\n","\n","        movi_loss_base = movi_cos_loss + movi_mse_loss # (0) for 1280\n","        eph_base_loss = movi_loss_base.mean().item()\n","\n","        # CosineEmbeddingLoss between difference\n","        cos_acc = cos_loss(pred_unclip_embs.squeeze(1).T,\n","                           img_embs[bs_idx].squeeze(1).T.to(DEVICE), (-1)*target).half()\n","\n","        eph_cos_acc+= cos_acc.mean().item()\n","\n","        del(time_base, directions, diff_img_embs, diff_unclip_embs)\n","\n","        to_rote, rote_norm, rote_back = 0, 0, 0\n","        if add_rote_train:\n","            # Rotation train steps\n","            rote_norm = len(config_norm['id_uclip_emb'])\n","            rote_back = len(config_back['id_uclip_emb'])\n","            to_rote =  rote_norm + rote_back\n","\n","        if add_rote_train and epoch and to_rote:\n","            text_hid_states_2rt = []\n","            unclip_embs_2rt = []\n","            base_img_embs_2rt = []\n","            image_embs_2rt = []\n","            delta_2rt = []\n","            direction_2rt = []\n","\n","            # intit class for shufflee again\n","            srs = Shuff_Reshuff(to_rote)\n","            s_idx = srs.shuffle() # shuffleed indexees\n","            u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","\n","            if rote_norm:\n","                # collect norm steps\n","                text_hid_states_2rt.append(torch.clone(text_hid_states[:d_batch])[config_norm['id_uclip_emb']])\n","                unclip_embs_2rt.append(torch.clone(base_unclip_embs[:d_batch])[config_norm['id_uclip_emb']])\n","                base_img_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_emb_s']])\n","                image_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_delta']])\n","                delta_2rt.append(torch.tensor(config_norm['delta']).unsqueeze(1))\n","                direction_2rt.append(torch.zeros_like(delta_2rt[-1]))\n","\n","\n","            if rote_back and add_back_train:\n","                # collect back steps\n","                text_hid_states_2rt.append(torch.clone(text_hid_states[d_batch:])[config_back['id_uclip_emb']])\n","                unclip_embs_2rt.append(torch.clone(base_unclip_embs[d_batch:])[config_back['id_uclip_emb']])\n","                base_img_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_emb_s']])\n","                image_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_delta']])\n","                delta_2rt.append(torch.tensor(config_back['delta']).unsqueeze(1))\n","                direction_2rt.append(torch.ones_like(delta_2rt[-1]))\n","\n","            # concat tansors\n","            if add_back_train:\n","              if len(direction_2rt) > 2*d_batch:\n","                R_batch = 2*d_batch\n","              else: R_batch = len(direction_2rt)\n","            else:\n","              if len(direction_2rt) > d_batch:\n","                R_batch = d_batch\n","              else: R_batch = len(direction_2rt)\n","\n","            # shufle and take R_batch samples\n","            text_hid_states_2rt = torch.concat(text_hid_states_2rt)[s_idx][:R_batch].to(DEVICE)\n","            unclip_embs_2rt = torch.concat(unclip_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","            base_img_embs_2rt = torch.concat(base_img_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","            image_embs_2rt = torch.concat(image_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","            delta_2rt = torch.concat(delta_2rt)[s_idx][s_idx][:R_batch].to(DEVICE)\n","            direction_2rt = torch.concat(direction_2rt)[s_idx][:R_batch].to(DEVICE)\n","\n","            take_base = base_img_embs_2rt.shape[0]\n","            # get cos_sim  base_img_embs vectors and base_unclip_embs vectors\n","            cos_sim = torch.cosine_similarity(base_img_embs[:take_base].unsqueeze(1), base_unclip_embs[:take_base], dim = 2)\n","\n","            # get rotation marixes_i2i\n","            R_marixes_i2i = RV.get_rotation_matrix(base_img_embs[:take_base].to(DEVICE), base_img_embs_2rt.squeeze(1)).to(DEVICE)\n","\n","            # compute roted unclip_embs with R_marixes_i2i and cos_sim base_img_embs and base_unclip_embs\n","            unclip_embs_2rt = RV.cosim_rotate(unclip_embs_2rt, cos_sim.to(DEVICE), R_marixes_i2i)\n","\n","            # get rotation marixes_u2u\n","            R_marixes_u2u = RV.get_rotation_matrix(base_unclip_embs[:take_base].squeeze(1).to(torch.float32).to(DEVICE),\n","                                                                  unclip_embs_2rt.squeeze(1))\n","\n","            # compute roted text_hid_states with R_marixes_u2u and cos_sim base_img_embs and base_unclip_embs\n","            text_hid_states_2rt = text_hid_states_2rt @ R_marixes_u2u\n","\n","\n","            # rotation predict\n","            pred_rote_embs = model(\n","                          text_hidden_states = text_hid_states_2rt.to(torch.float32),\n","                          prior_embeds = unclip_embs_2rt.to(torch.float32),\n","                          delta_time = delta_2rt,\n","                          direction = direction_2rt\n","                                            )\n","\n","            # difference\n","            diff_unclip_embeds = (unclip_embs_2rt.squeeze(dim=1) - pred_rote_embs.squeeze(dim=1))\n","            diff_img_embs = (base_img_embs_2rt.squeeze(dim=1) - image_embs_2rt.squeeze(dim=1))\n","\n","            rote_loss_weight =hist[\"acc\"][-1]\n","            #print(\"rote_loss_weight\", rote_loss_weight)\n","            # CosineEmbeddingLoss between difference\n","            movi_cos_loss += rote_loss_weight*cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  #\n","\n","            # MSELoss between predict and\n","            movi_mse_loss += rote_loss_weight*mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)).mean(0) #\n","\n","            #movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","            del(delta_2rt, direction_2rt, diff_img_embs, diff_unclip_embeds)\n","            del(pred_rote_embs, text_hid_states_2rt, R_marixes_i2i, R_marixes_u2u)\n","            del(unclip_embs_2rt,image_embs_2rt)\n","\n","        # Diff train steps\n","        to_diff = 0\n","        if add_diff_train:\n","            config_diff_norm, config_diff_back = cdp.getpoints_diftrain()\n","            diff_norm = len(config_diff_norm['id_uclip_emb'])\n","            diff_back = len(config_diff_back['id_uclip_emb'])\n","            to_diff = diff_norm + diff_back\n","\n","        if add_diff_train and epoch>1 and to_diff:\n","            diff_loss_weight = 1 - hist[\"base_loss\"][-1]/hist[\"base_loss\"][0]\n","            if diff_loss_weight<0: diff_loss_weight = 0\n","\n","        if add_diff_train and epoch>1 and to_diff and diff_loss_weight:\n","            take_text_hid_states = []\n","            take_base_unclip_embs = []\n","            next_unclip_embs = []\n","            next_base_img_embs = []\n","            next_image_embs = []\n","            next_delta = []\n","            next_text_hid_states = []\n","            next_direction = []\n","\n","            # un_shuffleed\n","            pred_unclip_embs = torch.clone(pred_unclip_embs.detach().cpu())[bu_idx]\n","\n","            # intit class for shufflee again\n","            srs = Shuff_Reshuff(to_diff)\n","            s_idx = srs.shuffle() # shuffleed indexees\n","            u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","            if diff_norm:\n","                # collect next norm steps\n","                take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","                take_text_hid_states.append(torch.clone(text_hid_states[:d_batch])[config_diff_norm['id_uclip_emb']])\n","                next_unclip_embs.append(torch.clone(pred_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","                next_base_img_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_emb_s']])\n","                next_image_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_delta']])\n","                next_delta.append(torch.tensor(config_diff_norm['delta']).unsqueeze(1))\n","                next_direction.append(torch.zeros_like(next_delta[-1]))\n","\n","            if diff_back:\n","                # collect next back steps\n","                take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_back['id_uclip_emb']])\n","                take_text_hid_states.append(torch.clone(text_hid_states[d_batch:])[config_diff_back['id_uclip_emb']])\n","                next_unclip_embs.append(torch.clone(pred_unclip_embs[d_batch:])[config_diff_back['id_uclip_emb']])\n","                next_base_img_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_emb_s']])\n","                next_image_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_delta']])\n","                next_delta.append(torch.tensor(config_diff_back['delta']).unsqueeze(1))\n","                next_direction.append(torch.ones_like(next_delta[-1]))\n","\n","\n","\n","            take_base_unclip_embs = torch.concat(take_base_unclip_embs).to(DEVICE)\n","            take_text_hid_states = torch.concat(take_text_hid_states).to(DEVICE)\n","            next_unclip_embs = torch.concat(next_unclip_embs).to(DEVICE)\n","            next_base_img_embs = torch.concat(next_base_img_embs)\n","            next_image_embs = torch.concat(next_image_embs)\n","            next_delta = torch.concat(next_delta)[s_idx].to(DEVICE)\n","            next_direction = torch.concat(next_direction)[s_idx].to(DEVICE)\n","\n","            # get rotation vectors\n","            R_marixes = RV.get_rotation_matrix(take_base_unclip_embs.squeeze(1), next_unclip_embs.squeeze(1))\n","            next_text_hid_states = take_text_hid_states @ R_marixes\n","\n","            # dif predict from base predict\n","            next_pred_unclip_embs = model(\n","                text_hidden_states = next_text_hid_states[s_idx].to(torch.float32),\n","                prior_embeds = next_unclip_embs[s_idx].to(torch.float32),\n","                delta_time = next_delta,\n","                direction = next_direction\n","                )\n","\n","            # difference\n","            diff_unclip_embeds = (next_unclip_embs[s_idx].squeeze(dim=1).to(DEVICE) - next_pred_unclip_embs.squeeze(dim=1))\n","            diff_img_embs =  (next_base_img_embs[s_idx].squeeze(dim=1) - next_image_embs[s_idx].squeeze(dim=1))\n","\n","\n","            # CosineEmbeddingLoss between difference\n","            movi_cos_loss += diff_loss_weight * cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n","\n","            # MSELoss between predict and\n","            movi_mse_loss += diff_loss_weight * mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)).mean(0) #\n","\n","            del(diff_img_embs, diff_unclip_embeds)\n","            del(next_image_embs, next_base_img_embs, next_delta, next_direction)\n","            del(next_text_hid_states,  next_unclip_embs)\n","            del(pred_unclip_embs, next_pred_unclip_embs)\n","\n","\n","    # collect loss\n","    movi_loss = movi_cos_loss + movi_mse_loss # (0) for 1280\n","    movi_loss.backward(torch.ones_like(movi_loss))\n","    eph_loss_mse += movi_mse_loss.mean().item()\n","    eph_loss_cos += movi_cos_loss.mean().item()\n","    eph_loss += movi_loss.mean().item()\n","\n","\n","    optimizer.step()\n","\n","    hist[\"lr\"].append(cur_lr)\n","    good_steps = len(all_data)\n","    eph_cos_acc/=good_steps\n","    eph_base_loss/=good_steps\n","    eph_loss/=good_steps\n","    eph_loss_mse/=good_steps\n","    eph_loss_cos/=good_steps\n","\n","    hist[\"loss\"].append(eph_loss)\n","    hist[\"loss_mse\"].append(eph_loss_mse)\n","    hist[\"loss_cos\"].append(eph_loss_cos)\n","    hist[\"acc\"].append(eph_cos_acc)\n","    hist[\"base_loss\"].append(eph_base_loss)\n","\n","    scheduler.step()\n","    flush_memory()\n","    cur_lr = scheduler.get_last_lr()[0]\n","    cur_lr = round(cur_lr, 5)\n","\n","    # https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n","    LR_RATE*=0.97\n","    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n","                                                  cycle_momentum = False,\n","                                                  base_lr=LR_RATE/10,\n","                                                  max_lr=LR_RATE,\n","                                                  step_size_up=5,\n","                                                  mode=\"triangular2\")\n","\n","    try: print(\"diff_loss_weight\", diff_loss_weight)\n","    except: pass\n","    print(f'\\rEp {epoch} all_loss {hist[\"loss\"][-1]:.5f} | acc {hist[\"acc\"][-1]:.5f} | mse_loss {hist[\"loss_mse\"][-1]:.5f} | cos_loss {hist[\"loss_cos\"][-1]:.5f} | lr {cur_lr:.5f}\\n')# {text}\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8696922233694517b2a451c2309245d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9eecd69d34394dc78d894c55ce063287","IPY_MODEL_2ad8acf31f614c52b85efe087dde4c83","IPY_MODEL_a9d63dd51dae4eb8b6e28a2512f014f7"],"layout":"IPY_MODEL_7d6325a736a34303ba71aecfcee00ac7"}},"9eecd69d34394dc78d894c55ce063287":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d3515d9ada34ef1bbd3be08eccbae90","placeholder":"​","style":"IPY_MODEL_8da9a3bccb354809a2843a245b98c9ba","value":"Пробегаемся по всем эпохам: 100%"}},"2ad8acf31f614c52b85efe087dde4c83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc4d1b3c124f4558a8cf2012fb253e14","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3cc581f3f73c41fa80b65b5454b1f224","value":3}},"a9d63dd51dae4eb8b6e28a2512f014f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b76180007f846808c6754a80801b855","placeholder":"​","style":"IPY_MODEL_4e134169a93b4d1289a7b5182bafe4d8","value":" 3/3 [00:52&lt;00:00, 19.98s/EPOHS]"}},"7d6325a736a34303ba71aecfcee00ac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d3515d9ada34ef1bbd3be08eccbae90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8da9a3bccb354809a2843a245b98c9ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc4d1b3c124f4558a8cf2012fb253e14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cc581f3f73c41fa80b65b5454b1f224":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b76180007f846808c6754a80801b855":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e134169a93b4d1289a7b5182bafe4d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}