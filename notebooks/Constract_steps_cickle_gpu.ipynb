{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzKZp1wyFLNmWixXxanDVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ddf5ae9168b4850b570e967afea554d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fb72a837d7042c89ac41355267b6ea9",
              "IPY_MODEL_df9c5898436247aca2e019e32ef4f4bf",
              "IPY_MODEL_188f9af29ffc4cb0a885ef5ec17bbe74"
            ],
            "layout": "IPY_MODEL_1a83df3cd5694eb6a42728c99cf4bcb5"
          }
        },
        "0fb72a837d7042c89ac41355267b6ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe2256088504ae8bab43f4e4b08d759",
            "placeholder": "​",
            "style": "IPY_MODEL_492e468e5ff54d1da0313241ed4a1441",
            "value": "Пробегаемся по всем эпохам: 100%"
          }
        },
        "df9c5898436247aca2e019e32ef4f4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f1e0dfebb3746fcb1750cfed0aa6677",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88e1945e69dc4acdaa163e4790c92f4e",
            "value": 10
          }
        },
        "188f9af29ffc4cb0a885ef5ec17bbe74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c8880bd27c4c76892928f715c9af40",
            "placeholder": "​",
            "style": "IPY_MODEL_d411f55c675940a7960eec318dbc88fd",
            "value": " 10/10 [04:48&lt;00:00, 28.66s/EPOHS]"
          }
        },
        "1a83df3cd5694eb6a42728c99cf4bcb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe2256088504ae8bab43f4e4b08d759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492e468e5ff54d1da0313241ed4a1441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f1e0dfebb3746fcb1750cfed0aa6677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e1945e69dc4acdaa163e4790c92f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26c8880bd27c4c76892928f715c9af40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d411f55c675940a7960eec318dbc88fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mike030668/Spliter_Text2Movi_Kandinskiy_22/blob/main/notebooks/Constract_steps_cickle_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dCFBzIDVjLQ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Mike030668/Spliter_Text2Movi_Kandinskiy_22.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct method to find points of start and end fragments movi"
      ],
      "metadata": {
        "id": "TJNOkxoJJOG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAIN_DIR =  \"/content/Spliter_Text2Movi_Kandinskiy_22\"\n",
        "import sys\n",
        "sys.path.append(MAIN_DIR)"
      ],
      "metadata": {
        "id": "WRSn86u98HGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## init rand data"
      ],
      "metadata": {
        "id": "zB0Kzp-DakPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utills.step_points import ComputeDiffPoints, Shuff_Reshuff"
      ],
      "metadata": {
        "id": "JQ8htmqkWz0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdp = ComputeDiffPoints()\n",
        "\n",
        "# make set rand embeddings for tests\n",
        "d_batch = 15\n",
        "MAX_LEN_MOVI = 100\n",
        "\n",
        "# sets images embeds\n",
        "image_embeds = torch.randn(d_batch+1, 1, 1280)\n",
        "\n",
        "# text embedings prompt video from Text model\n",
        "text_hid_state = torch.randn((1, 77, 1280), requires_grad=True)\n",
        "\n",
        "# unclip embedings prompt video from Prior\n",
        "unclip_embed = torch.randn((1, 1, 1280), requires_grad=True)\n",
        "\n",
        "# make set time_labels\n",
        "labels = np.random.choice(np.arange(1, MAX_LEN_MOVI), d_batch+1, replace=False)\n",
        "\n",
        "# start and end points to use for start normal and back ways\n",
        "labels[0] = 0\n",
        "labels[-1] = MAX_LEN_MOVI-1\n",
        "\n",
        "print(labels)\n",
        "cdp.time_labels = labels\n",
        "config_norm, config_back = cdp.getpoints_train()\n",
        "\n",
        "print(f\"Time labels {labels}\\n\")\n",
        "print('\\nNorm_way')\n",
        "print(f\"Start base point {cdp.s_point}\")\n",
        "print(f\"Base normal points {cdp.points}\\n\\n\")\n",
        "print('\\nBack_way')\n",
        "print(f\"Start base back_point {cdp.back_s_point}\")\n",
        "print(f\"Base back points {cdp.back_points}\\n\\n\")"
      ],
      "metadata": {
        "id": "_3aRPe-3Y5Ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16925e27-e3a0-4fd0-e2b1-ff2a754789eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0 15 11 38 17  1 47 76 16 27 29 89 40 20 12 99]\n",
            "Time labels [ 0 15 11 38 17  1 47 76 16 27 29 89 40 20 12 99]\n",
            "\n",
            "\n",
            "Norm_way\n",
            "Start base point 0\n",
            "Base normal points [15 11 38 17  1 47 76 16 27 29 89 40 20 12 99]\n",
            "\n",
            "\n",
            "\n",
            "Back_way\n",
            "Start base back_point 0\n",
            "Base back points [87 79 59 10 70 72 83 23 52 98 82 61 88 84 99]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compute normal and back points for rotation steps"
      ],
      "metadata": {
        "id": "-Oq9QFVcaqLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time labels {labels}\")\n",
        "print('\\n\\nNorm rotation way')\n",
        "print(f\"Base normal points {cdp.points} for start rotation steps \\n\")\n",
        "id_img_emb_s = config_norm[\"id_img_emb_s\"]\n",
        "print(f\"ID image and text start rotation points {id_img_emb_s}\")\n",
        "id_uclip_emb = config_norm[\"id_uclip_emb\"]\n",
        "print(f\"ID uclip_emb start rotation points {id_uclip_emb}\")\n",
        "id_img_delta = config_norm[\"id_img_delta\"]\n",
        "print(f\"ID delta rotation points {id_img_delta}\")\n",
        "delta = config_norm[\"delta\"]\n",
        "print(f\"Delta between points {delta} normal points\")\n",
        "\n",
        "\n",
        "print('\\n\\nNorm rotation way')\n",
        "print(f\"Base back points {cdp.back_points} for start rotation back steps \\n\")\n",
        "id_img_emb_s = config_back[\"id_img_emb_s\"]\n",
        "print(f\"ID image and text start rotation points {id_img_emb_s} in base back points\")\n",
        "id_uclip_emb = config_back[\"id_uclip_emb\"]\n",
        "print(f\"ID uclip_emb start rotation points {id_uclip_emb} in base back points\")\n",
        "id_img_delta = config_back[\"id_img_delta\"]\n",
        "print(f\"ID delta rotation points {id_img_delta} in base back points\")\n",
        "delta = config_back[\"delta\"]\n",
        "print(f\"Delta between points {delta} back points\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ea874b-2565-48c6-f170-45eac8f3198e",
        "id": "_Qp3aR2hurQB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time labels [ 0 15 11 38 17  1 47 76 16 27 29 89 40 20 12 99]\n",
            "\n",
            "\n",
            "Norm rotation way\n",
            "Base normal points [15 11 38 17  1 47 76 16 27 29 89 40 20 12 99] for start rotation steps \n",
            "\n",
            "ID image and text start rotation points [1, 4, 13, 4, 0, 1, 3, 4, 7, 8, 9, 12, 13, 0, 1, 4, 7, 13, 0, 1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 0, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 0, 1, 4, 13, 0, 1, 3, 4, 7, 12, 13, 0, 1, 3, 4, 7, 8, 12, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 0, 1, 2, 3, 4, 7, 8, 9, 12, 13, 0, 1, 3, 4, 7, 13, 1, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
            "ID uclip_emb start rotation points [1, 4, 13, 4, 0, 1, 3, 4, 7, 8, 9, 12, 13, 0, 1, 4, 7, 13, 0, 1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 0, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 0, 1, 4, 13, 0, 1, 3, 4, 7, 12, 13, 0, 1, 3, 4, 7, 8, 12, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 0, 1, 2, 3, 4, 7, 8, 9, 12, 13, 0, 1, 3, 4, 7, 13, 1, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
            "ID delta rotation points [0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "Delta between points [4, 14, 3, 10, 23, 27, 21, 37, 22, 11, 9, 18, 26, 2, 6, 16, 1, 5, 32, 36, 9, 30, 46, 31, 20, 18, 7, 27, 35, 61, 65, 38, 59, 75, 29, 60, 49, 47, 36, 56, 64, 1, 5, 15, 4, 12, 16, 10, 26, 11, 7, 15, 14, 18, 12, 28, 13, 2, 9, 17, 74, 78, 51, 72, 88, 42, 13, 73, 62, 60, 49, 69, 77, 25, 29, 2, 23, 39, 24, 13, 11, 20, 28, 5, 9, 3, 19, 4, 8, 1, 11, 84, 88, 61, 82, 98, 52, 23, 83, 72, 70, 10, 59, 79, 87] normal points\n",
            "\n",
            "\n",
            "Norm rotation way\n",
            "Base back points [87 79 59 10 70 72 83 23 52 98 82 61 88 84 99] for start rotation back steps \n",
            "\n",
            "ID image and text start rotation points [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 2, 3, 4, 5, 7, 8, 11, 3, 7, 8, 2, 3, 7, 8, 11, 2, 3, 4, 7, 8, 11, 1, 2, 3, 4, 5, 7, 8, 10, 11, 3, 3, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 1, 2, 3, 4, 5, 7, 8, 11, 2, 3, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] in base back points\n",
            "ID uclip_emb start rotation points [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 2, 3, 4, 5, 7, 8, 11, 3, 7, 8, 2, 3, 7, 8, 11, 2, 3, 4, 7, 8, 11, 1, 2, 3, 4, 5, 7, 8, 10, 11, 3, 3, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 1, 2, 3, 4, 5, 7, 8, 11, 2, 3, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] in base back points\n",
            "ID delta rotation points [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14] in base back points\n",
            "Delta between points [8, 28, 77, 17, 15, 4, 64, 35, 5, 26, 3, 20, 69, 9, 7, 56, 27, 18, 49, 36, 7, 11, 60, 47, 18, 9, 13, 62, 2, 49, 20, 11, 4, 24, 73, 13, 11, 60, 31, 1, 22, 13, 42, 29, 11, 19, 39, 88, 28, 26, 15, 75, 46, 16, 37, 10, 14, 3, 23, 72, 12, 10, 59, 30, 21, 2, 51, 38, 9, 1, 9, 29, 78, 18, 16, 5, 65, 36, 6, 27, 4, 5, 25, 74, 14, 12, 1, 61, 32, 2, 23, 12, 20, 40, 89, 29, 27, 16, 76, 47, 1, 17, 38, 11, 15] back points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compute normal and back points for diff steps\n",
        "\n",
        "It will be used for prediction from prediction of model"
      ],
      "metadata": {
        "id": "Tp7w3zw7TmDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_diff_norm, config_diff_back = cdp.getpoints_diftrain()\n",
        "\n",
        "print(f\"Time labels {labels}\")\n",
        "print('\\n\\nNorm diff way')\n",
        "print(f\"Base normal points {cdp.points}\")\n",
        "print(f\"Next normal points {cdp.next_points} for diff steps \\n\")\n",
        "id_img_emb_s = config_diff_norm[\"id_img_emb_s\"]\n",
        "print(f\"ID image and text start diff points {id_img_emb_s} in base normal points\")\n",
        "id_uclip_emb = config_diff_norm[\"id_uclip_emb\"]\n",
        "print(f\"ID uclip_emb start diff points {id_uclip_emb} in next normal points\")\n",
        "id_img_delta = config_diff_norm[\"id_img_delta\"]\n",
        "print(f\"ID delta diff points {id_img_delta} in next normal points\")\n",
        "delta = config_diff_norm[\"delta\"]\n",
        "print(f\"Delta between diff points {delta} in base normal points\")\n",
        "\n",
        "\n",
        "print('\\n\\nNorm rotation way')\n",
        "print(f\"Base back points {cdp.back_points}\")\n",
        "print(f\"Next back points {cdp.back_next_points} for diff steps \\n\")\n",
        "id_img_emb_s = config_diff_back[\"id_img_emb_s\"]\n",
        "print(f\"ID image and text start diff points {id_img_emb_s} in base back points\")\n",
        "id_uclip_emb = config_diff_back[\"id_uclip_emb\"]\n",
        "print(f\"ID uclip_emb start diff points {id_uclip_emb} in next back points\")\n",
        "id_img_delta = config_diff_back[\"id_img_delta\"]\n",
        "print(f\"ID delta diff points {id_img_delta} in next back points\")\n",
        "delta = config_diff_back[\"delta\"]\n",
        "print(f\"Delta between diff points {delta} back points\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479f8259-07bc-400d-b9de-7d0022b490b0",
        "id": "D2Q7stqHZbbM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time labels [ 0 15 11 38 17  1 47 76 16 27 29 89 40 20 12 99]\n",
            "\n",
            "\n",
            "Norm diff way\n",
            "Base normal points [15 11 38 17  1 47 76 16 27 29 89 40 20 12 99]\n",
            "Next normal points [ 16  12  39  18   2  48  77  17  28  30  90  41  21  13 100] for diff steps \n",
            "\n",
            "ID image and text start diff points [7, 13, 13] in base normal points\n",
            "ID uclip_emb start diff points [0, 1, 1] in next normal points\n",
            "ID delta diff points [3, 3, 7] in next normal points\n",
            "Delta between diff points [1, 5, 4] in base normal points\n",
            "\n",
            "\n",
            "Norm rotation way\n",
            "Base back points [87 79 59 10 70 72 83 23 52 98 82 61 88 84 99]\n",
            "Next back points [ 88  80  60  11  71  73  84  24  53  99  83  62  89  85 100] for diff steps \n",
            "\n",
            "ID image and text start diff points [6, 13, 6, 6, 12, 13] in base back points\n",
            "ID uclip_emb start diff points [10, 6, 10, 10, 0, 6] in next back points\n",
            "ID delta diff points [12, 12, 13, 14, 14, 14] in next back points\n",
            "Delta between diff points [5, 4, 1, 16, 11, 15] back points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Both ways steps"
      ],
      "metadata": {
        "id": "eRPXhwREGqQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.baseline import SpliterSimple\n",
        "from utills.clearing import flush_memory\n",
        "\n",
        "model = SpliterSimple(max_delta_time = 100,\n",
        "                        emb_dim = 1280,\n",
        "                        ways = 2\n",
        "                        )"
      ],
      "metadata": {
        "id": "EH5Kf3UVY3gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses\n",
        "\n",
        "nn.CosineEmbeddingLoss\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html"
      ],
      "metadata": {
        "id": "fLGgqgDL778b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_text_last_hidden_state = torch.randn((2, 77, 1280)) #outputs.last_hidden_state\n",
        "embeddings = torch.randn((2, 1, 1280))\n",
        "delta = torch.tensor([[3],[5]])\n",
        "direction  = torch.tensor([[0],[1]])\n",
        "model.to(DEVICE)\n",
        "out_embeddings = model(text_hidden_states = clip_text_last_hidden_state.to(DEVICE),\n",
        "                       prior_embeds = embeddings.to(DEVICE),\n",
        "                       delta_time = delta.to(DEVICE),\n",
        "                       direction = direction.to(DEVICE)\n",
        "                        )\n",
        "out_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l05KihJir06z",
        "outputId": "cef8d3fa-3094-4037-c3b9-bcf5754068e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 1280])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\n",
        "loss = nn.CosineEmbeddingLoss()\n",
        "input1 = torch.randn(3, 1, requires_grad=True)\n",
        "input2 = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.ones(3)\n",
        "output = loss(input1, input2, target)\n",
        "output.backward()\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRthV5Nc5bHJ",
        "outputId": "f731f9f9-e6f0-4a2b-c69e-44dd14b073cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5295, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apply as vector losses"
      ],
      "metadata": {
        "id": "GVtldMSOUWx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss = nn.MSELoss(reduction='none').to(DEVICE)\n",
        "cos_loss = nn.CosineEmbeddingLoss(reduction = 'none').to(DEVICE)\n",
        "target = torch.ones(1280).to(DEVICE)\n",
        "loss_cos = cos_loss(embeddings.squeeze(1).T.to(DEVICE), out_embeddings.squeeze(1).T, target)\n",
        "print(loss_cos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g-uuGIc6Zm_",
        "outputId": "96dc2373-6e48-4fd8-c909-e83c91c2f2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.7729, 0.5990, 0.0446,  ..., 1.2958, 0.9652, 0.7654], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## first steps normal and back shuffleed"
      ],
      "metadata": {
        "id": "cL-QqPMYGqQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_points = np.append(cdp.points, cdp.back_points)\n",
        "# intit class for shufflee\n",
        "b_srs = Shuff_Reshuff(len(all_points))\n",
        "bs_idx = b_srs.shuffle() # shuffleed indexees\n",
        "bu_idx = b_srs.unshuffle() # indexees for re_shuffle\n",
        "\n",
        "B_batch = 2*d_batch\n",
        "\n",
        "time_base = torch.tensor(all_points).unsqueeze(1)\n",
        "directions = torch.concat([torch.zeros_like(time_base[:d_batch]), torch.ones_like(time_base[d_batch:])])\n",
        "\n",
        "# shuffleed\n",
        "time_base = time_base.to(DEVICE)\n",
        "directions = directions.to(DEVICE)\n",
        "\n",
        "\n",
        "text_hid_states =  torch.concat([text_hid_state for _ in range(B_batch)])\n",
        "base_unclip_embs = torch.concat([unclip_embed for _ in range(B_batch)])\n",
        "\n",
        "base_img_embs = [image_embeds[0] for _ in range(d_batch)] + [image_embeds[-1] for _ in range(d_batch)]\n",
        "base_img_embs = torch.concat(base_img_embs)\n",
        "\n",
        "\n",
        "back_img_embs = torch.concat([image_embeds[i,:,:] for i in range(image_embeds.shape[0])][::-1]).unsqueeze(1)\n",
        "img_embs =[image_embeds[1:,:,:], back_img_embs[1:,:,:]]\n",
        "img_embs = torch.concat(img_embs)\n",
        "\n",
        "\n",
        "pred_unclip_embs = model(\n",
        "      text_hidden_states = text_hid_states[bs_idx].to(torch.float32).to(DEVICE), # shuffleed\n",
        "      prior_embeds = base_unclip_embs[bs_idx].to(torch.float32).to(DEVICE),\n",
        "      delta_time = time_base[bs_idx],\n",
        "      direction = directions[bs_idx]\n",
        "                        )\n",
        "\n",
        "# difference\n",
        "diff_img_embs =  (base_img_embs[bs_idx].squeeze(dim=1) - img_embs[bs_idx].squeeze(dim=1)) #\n",
        "diff_unclip_embs = (base_unclip_embs[bs_idx].squeeze(dim=1).to(DEVICE) - pred_unclip_embs.squeeze(dim=1))\n",
        "\n",
        "# CosineEmbeddingLoss between difference\n",
        "movi_cos_loss = cos_loss(diff_unclip_embs.T, diff_img_embs.T.to(DEVICE), target).half()  #\n",
        "\n",
        "# MSELoss between predict and\n",
        "movi_mse_loss = mse_loss(diff_unclip_embs.half(), diff_img_embs.half().to(DEVICE))#\n",
        "\n",
        "movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n",
        "\n",
        "del(time_base, directions, diff_img_embs, diff_unclip_embs)\n",
        "flush_memory()\n",
        "\n",
        "print(movi_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4906b8-bbe8-4521-b3b5-54eec31b2349",
        "id": "mO_PB6bQGqQJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.5410, 2.2637, 1.9648,  ..., 2.2578, 4.3359, 2.6211], device='cuda:0',\n",
            "       dtype=torch.float16, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotation steps normal and back shuffleed"
      ],
      "metadata": {
        "id": "zWh3qG0Nseff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utills.rotations import RotationVectors"
      ],
      "metadata": {
        "id": "7FgvF74Dcjb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inite rotation class\n",
        "RV = RotationVectors()"
      ],
      "metadata": {
        "id": "IF4rY1xAwInw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rote_norm_train = len(config_norm['id_uclip_emb'])\n",
        "rote_back_train = len(config_back['id_uclip_emb'])\n",
        "rote_train =  rote_norm_train + rote_back_train\n",
        "\n",
        "if rote_train:\n",
        "    text_hid_states_2rt = []\n",
        "    unclip_embs_2rt = []\n",
        "    base_img_embs_2rt = []\n",
        "    image_embs_2rt = []\n",
        "    delta_2rt = []\n",
        "    direction_2rt = []\n",
        "\n",
        "    # intit class for shufflee again\n",
        "    srs = Shuff_Reshuff(rote_train)\n",
        "    s_idx = srs.shuffle() # shuffleed indexees\n",
        "    u_idx = srs.unshuffle() # indexees for re_shuffle\n",
        "\n",
        "\n",
        "    if rote_norm_train:\n",
        "        # collect norm steps\n",
        "        text_hid_states_2rt.append(torch.clone(text_hid_states[:d_batch])[config_norm['id_uclip_emb']])\n",
        "        unclip_embs_2rt.append(torch.clone(base_unclip_embs[:d_batch])[config_norm['id_uclip_emb']])\n",
        "        base_img_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_emb_s']])\n",
        "        image_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_delta']])\n",
        "        delta_2rt.append(torch.tensor(config_norm['delta']).unsqueeze(1))\n",
        "        direction_2rt.append(torch.zeros_like(delta_2rt[-1]))\n",
        "\n",
        "\n",
        "    if rote_back_train:\n",
        "        # collect back steps\n",
        "        text_hid_states_2rt.append(torch.clone(text_hid_states[d_batch:])[config_back['id_uclip_emb']])\n",
        "        unclip_embs_2rt.append(torch.clone(base_unclip_embs[d_batch:])[config_back['id_uclip_emb']])\n",
        "        base_img_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_emb_s']])\n",
        "        image_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_delta']])\n",
        "        delta_2rt.append(torch.tensor(config_back['delta']).unsqueeze(1))\n",
        "        direction_2rt.append(torch.ones_like(delta_2rt[-1]))\n",
        "\n",
        "    # concat tansors\n",
        "    if rote_train > B_batch:\n",
        "       R_batch = B_batch\n",
        "    else: R_batch = rote_train\n",
        "\n",
        "    # shufle and take R_batch samples\n",
        "    text_hid_states_2rt = torch.concat(text_hid_states_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "    unclip_embs_2rt = torch.concat(unclip_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "    base_img_embs_2rt = torch.concat(base_img_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "    image_embs_2rt = torch.concat(image_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "    delta_2rt = torch.concat(delta_2rt)[s_idx][s_idx][:R_batch].to(DEVICE)\n",
        "    direction_2rt = torch.concat(direction_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "\n",
        "    # get rotation vectors\n",
        "    R_marixes = RV.get_rotation_matrix(base_img_embs_2rt.squeeze(1), image_embs_2rt.squeeze(1)).to(DEVICE)\n",
        "    text_hid_states_2rt = torch.bmm(text_hid_states_2rt, R_marixes)\n",
        "    unclip_embs_2rt = torch.bmm(unclip_embs_2rt, R_marixes)\n",
        "\n",
        "    pred_rote_embs = model(\n",
        "                  text_hidden_states = text_hid_states_2rt.to(torch.float32), # shuffleed\n",
        "                  prior_embeds = unclip_embs_2rt.to(torch.float32),\n",
        "                  delta_time = delta_2rt,\n",
        "                  direction = direction_2rt\n",
        "                                    )\n",
        "\n",
        "    # difference\n",
        "    diff_unclip_embeds = (unclip_embs_2rt.squeeze(dim=1) - pred_rote_embs.squeeze(dim=1))\n",
        "    diff_img_embs =  (base_img_embs_2rt.squeeze(dim=1) - image_embs_2rt.squeeze(dim=1))\n",
        "\n",
        "    # CosineEmbeddingLoss between difference\n",
        "    movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n",
        "\n",
        "    # MSELoss between predict and\n",
        "    movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n",
        "\n",
        "    movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n",
        "\n",
        "    del(delta_2rt, direction_2rt, diff_img_embs, diff_unclip_embeds)\n",
        "    del(pred_rote_embs, text_hid_states_2rt, R_marixes)\n",
        "    del(unclip_embs_2rt,image_embs_2rt)\n",
        "    flush_memory()\n",
        "\n",
        "\n",
        "    print(movi_loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uY7utEtutNJ",
        "outputId": "152b262f-394f-4d0f-c8f5-3e6711009c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.4102, 4.2930, 5.7227,  ..., 5.4375, 7.8203, 5.3281], device='cuda:0',\n",
            "       dtype=torch.float16, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps normal and back from pred steps shuffleed"
      ],
      "metadata": {
        "id": "FEz3tuuKBMHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff_norm = len(config_diff_norm['id_uclip_emb'])\n",
        "diff_back = len(config_diff_back['id_uclip_emb'])\n",
        "diff_train = diff_norm + diff_back\n",
        "\n",
        "if diff_train:\n",
        "    take_text_hid_states = []\n",
        "    take_base_unclip_embs = []\n",
        "    next_unclip_embs = []\n",
        "    next_base_img_embs = []\n",
        "    next_image_embs = []\n",
        "    next_delta = []\n",
        "    next_text_hid_states = []\n",
        "    next_direction = []\n",
        "\n",
        "    # un_shuffleed\n",
        "    pred_unclip_embs = torch.clone(pred_unclip_embs.detach().cpu())[bu_idx]\n",
        "\n",
        "    # intit class for shufflee again\n",
        "    srs = Shuff_Reshuff(diff_train)\n",
        "    s_idx = srs.shuffle() # shuffleed indexees\n",
        "    u_idx = srs.unshuffle() # indexees for re_shuffle\n",
        "\n",
        "    if diff_norm:\n",
        "        # collect next norm steps\n",
        "        take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n",
        "        take_text_hid_states.append(torch.clone(text_hid_states[:d_batch])[config_diff_norm['id_uclip_emb']])\n",
        "        next_unclip_embs.append(torch.clone(pred_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n",
        "        next_base_img_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_emb_s']])\n",
        "        next_image_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_delta']])\n",
        "        next_delta.append(torch.tensor(config_diff_norm['delta']).unsqueeze(1))\n",
        "        next_direction.append(torch.zeros_like(next_delta[-1]))\n",
        "\n",
        "    if diff_back:\n",
        "        # collect next back steps\n",
        "        take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_back['id_uclip_emb']])\n",
        "        take_text_hid_states.append(torch.clone(text_hid_states[d_batch:])[config_diff_back['id_uclip_emb']])\n",
        "        next_unclip_embs.append(torch.clone(pred_unclip_embs[d_batch:])[config_diff_back['id_uclip_emb']])\n",
        "        next_base_img_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_emb_s']])\n",
        "        next_image_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_delta']])\n",
        "        next_delta.append(torch.tensor(config_diff_back['delta']).unsqueeze(1))\n",
        "        next_direction.append(torch.ones_like(next_delta[-1]))\n",
        "\n",
        "\n",
        "\n",
        "    take_base_unclip_embs = torch.concat(take_base_unclip_embs).to(DEVICE)\n",
        "    take_text_hid_states = torch.concat(take_text_hid_states).to(DEVICE)\n",
        "    next_unclip_embs = torch.concat(next_unclip_embs).to(DEVICE)\n",
        "    next_base_img_embs = torch.concat(next_base_img_embs)\n",
        "    next_image_embs = torch.concat(next_image_embs)\n",
        "    next_delta = torch.concat(next_delta)[s_idx].to(DEVICE)\n",
        "    next_direction = torch.concat(next_direction)[s_idx].to(DEVICE)\n",
        "\n",
        "    # get rotation vectors\n",
        "    R_marixes = RV.get_rotation_matrix(take_base_unclip_embs.squeeze(1), next_unclip_embs.squeeze(1))\n",
        "    next_text_hid_states = torch.bmm(take_text_hid_states, R_marixes)\n",
        "\n",
        "\n",
        "    next_pred_unclip_embs = model(\n",
        "        text_hidden_states = next_text_hid_states[s_idx].to(torch.float32),\n",
        "        prior_embeds = next_unclip_embs[s_idx].to(torch.float32),\n",
        "        delta_time = next_delta,\n",
        "        direction = next_direction\n",
        "        )\n",
        "\n",
        "    # difference\n",
        "    diff_unclip_embeds = (next_unclip_embs[s_idx].squeeze(dim=1).to(DEVICE) - next_pred_unclip_embs.squeeze(dim=1))\n",
        "    diff_img_embs =  (next_base_img_embs[s_idx].squeeze(dim=1) - next_image_embs[s_idx].squeeze(dim=1))\n",
        "\n",
        "    # CosineEmbeddingLoss between difference\n",
        "    movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n",
        "\n",
        "    # MSELoss between predict and\n",
        "    movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n",
        "\n",
        "    next_movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n",
        "\n",
        "    del(diff_img_embs, diff_unclip_embeds)\n",
        "    del(next_image_embs, next_base_img_embs, next_delta, next_direction)\n",
        "    del(next_text_hid_states,  next_unclip_embs)\n",
        "    del(pred_unclip_embs, next_pred_unclip_embs)\n",
        "    flush_memory()\n",
        "\n",
        "    print(next_movi_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e351be79-3fb0-48aa-9dc7-8b25242ef675",
        "id": "zQg68d4NGqQJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.7500, 1.7344, 3.4199,  ..., 0.8779, 1.7559, 2.7773], device='cuda:0',\n",
            "       dtype=torch.float16, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Cicle"
      ],
      "metadata": {
        "id": "XW6o9w1mOQfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### init rand data"
      ],
      "metadata": {
        "id": "u2c2tlUzNLRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "# make set rand embeddings for tests\n",
        "d_batch = 15\n",
        "q_ty = 100\n",
        "\n",
        "for i in range(q_ty):\n",
        "    data = dict()\n",
        "    # make set time_labels\n",
        "    labels = np.random.choice(np.arange(1, 100), d_batch+1, replace=False)\n",
        "    labels[0] = 0\n",
        "    labels[-1] = 100\n",
        "    data['labels'] = labels\n",
        "\n",
        "    data['image_embeds'] = []\n",
        "    for i in range(d_batch+1):\n",
        "      if not i : data['image_embeds'].append(torch.randn(1, 1, 1280))\n",
        "      else:\n",
        "        next = data['image_embeds'][-1] + 0.0001*torch.randn(1, 1, 1280)\n",
        "        data['image_embeds'].append(next)\n",
        "\n",
        "    data['text_hid_state'] = torch.randn((1, 77, 1280))\n",
        "    data['unclip_embed'] = torch.randn((1, 1, 1280), requires_grad=True)\n",
        "\n",
        "    all_data.append(data)\n"
      ],
      "metadata": {
        "id": "j8ekLAdPOQfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LR_RATE = 1e-03\n",
        "EPS = 0.95\n",
        "\n",
        "mse_loss = nn.MSELoss(reduction='none').to(DEVICE)\n",
        "cos_loss = nn.CosineEmbeddingLoss(reduction = 'none').to(DEVICE)\n",
        "target = torch.ones(1280).to(DEVICE)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_RATE)\n",
        "\n",
        "# https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
        "                                              cycle_momentum = False,\n",
        "                                              base_lr=LR_RATE/10,\n",
        "                                              max_lr=LR_RATE,\n",
        "                                              step_size_up=5,\n",
        "                                              mode=\"triangular2\")\n",
        "model.train()\n",
        "\n",
        "# inite rotation class\n",
        "RV = RotationVectors()"
      ],
      "metadata": {
        "id": "5MrmU4BvOQfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test train"
      ],
      "metadata": {
        "id": "ZWAmLlguOEtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# отрисовка прохождения цикла\n",
        "from tqdm.notebook import tqdm\n",
        "from random import shuffle\n",
        "import time\n",
        "\n",
        "step = 0\n",
        "EPOHS = 10\n",
        "\n",
        "for ep  in tqdm(range(EPOHS), unit =\"EPOHS\",\n",
        "                      desc =\"Пробегаемся по всем эпохам\"):\n",
        "\n",
        "    shuffle(all_data)\n",
        "    for movi in all_data:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # make set rand embeddings for tests\n",
        "        image_embeds = torch.concat(movi['image_embeds'])\n",
        "        text_hid_state = movi['text_hid_state']\n",
        "        unclip_embed = movi['unclip_embed']\n",
        "        # make set time_labels\n",
        "        labels = movi['labels']\n",
        "\n",
        "        # place labels to class points\n",
        "        cdp.time_labels = labels\n",
        "        all_points = np.append(cdp.points, cdp.back_points)\n",
        "\n",
        "        # intit class for shufflee\n",
        "        b_srs = Shuff_Reshuff(len(all_points))\n",
        "        bs_idx = b_srs.shuffle() # shuffleed indexees\n",
        "        bu_idx = b_srs.unshuffle() # indexees for re_shuffle\n",
        "\n",
        "        B_batch = 2*d_batch\n",
        "\n",
        "        # collect time directions tensors\n",
        "        time_base = torch.tensor(all_points).unsqueeze(1)\n",
        "        directions = torch.concat([torch.zeros_like(time_base[:d_batch]),\n",
        "                                   torch.ones_like(time_base[d_batch:])])\n",
        "        # shuffleed\n",
        "        time_base = time_base.to(DEVICE)\n",
        "        directions = directions.to(DEVICE)\n",
        "\n",
        "        # collect text and unclip tensors\n",
        "        text_hid_states =  torch.concat([text_hid_state for _ in range(B_batch)])\n",
        "        base_unclip_embs = torch.concat([unclip_embed for _ in range(B_batch)])\n",
        "\n",
        "        # collect base_img_embs tensors\n",
        "        base_img_embs = [image_embeds[0] for _ in range(d_batch)] \\\n",
        "        + [image_embeds[-1] for _ in range(d_batch)]\n",
        "        base_img_embs = torch.concat(base_img_embs)\n",
        "\n",
        "        # collect back_img_embs tensors\n",
        "        back_img_embs = torch.concat([image_embeds[i,:,:] for i in range(image_embeds.shape[0])][::-1]).unsqueeze(1)\n",
        "        # collect img_embs together tensors\n",
        "        img_embs =[image_embeds[1:,:,:], back_img_embs[1:,:,:]]\n",
        "        img_embs = torch.concat(img_embs)\n",
        "\n",
        "        # base predict which can used in next step\n",
        "        pred_unclip_embs = model(\n",
        "              text_hidden_states = text_hid_states[bs_idx].to(torch.float32).to(DEVICE), # shuffleed\n",
        "              prior_embeds = base_unclip_embs[bs_idx].to(torch.float32).to(DEVICE),\n",
        "              delta_time = time_base[bs_idx],\n",
        "              direction = directions[bs_idx]\n",
        "                                )\n",
        "\n",
        "        # difference\n",
        "        diff_img_embs = (base_img_embs[bs_idx].squeeze(dim=1) - img_embs[bs_idx].squeeze(dim=1)) #\n",
        "        diff_unclip_embs = (base_unclip_embs[bs_idx].squeeze(dim=1).to(DEVICE) - pred_unclip_embs.squeeze(dim=1))\n",
        "\n",
        "        # CosineEmbeddingLoss between difference\n",
        "        movi_cos_loss = cos_loss(diff_unclip_embs.T, diff_img_embs.T.to(DEVICE), target).half()  #\n",
        "\n",
        "        # MSELoss between predict and\n",
        "        movi_mse_loss = mse_loss(diff_unclip_embs.half(), diff_img_embs.half().to(DEVICE))#\n",
        "\n",
        "        movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n",
        "\n",
        "        del(time_base, directions, diff_img_embs, diff_unclip_embs)\n",
        "\n",
        "\n",
        "        # Rotation train steps\n",
        "        rote_norm_train = len(config_norm['id_uclip_emb'])\n",
        "        rote_back_train = len(config_back['id_uclip_emb'])\n",
        "        rote_train =  rote_norm_train + rote_back_train\n",
        "\n",
        "        if rote_train:\n",
        "            text_hid_states_2rt = []\n",
        "            unclip_embs_2rt = []\n",
        "            base_img_embs_2rt = []\n",
        "            image_embs_2rt = []\n",
        "            delta_2rt = []\n",
        "            direction_2rt = []\n",
        "\n",
        "            # intit class for shufflee again\n",
        "            srs = Shuff_Reshuff(rote_train)\n",
        "            s_idx = srs.shuffle() # shuffleed indexees\n",
        "            u_idx = srs.unshuffle() # indexees for re_shuffle\n",
        "\n",
        "\n",
        "            if rote_norm_train:\n",
        "                # collect norm steps\n",
        "                text_hid_states_2rt.append(torch.clone(text_hid_states[:d_batch])[config_norm['id_uclip_emb']])\n",
        "                unclip_embs_2rt.append(torch.clone(base_unclip_embs[:d_batch])[config_norm['id_uclip_emb']])\n",
        "                base_img_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_emb_s']])\n",
        "                image_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_delta']])\n",
        "                delta_2rt.append(torch.tensor(config_norm['delta']).unsqueeze(1))\n",
        "                direction_2rt.append(torch.zeros_like(delta_2rt[-1]))\n",
        "\n",
        "\n",
        "            if rote_back_train:\n",
        "                # collect back steps\n",
        "                text_hid_states_2rt.append(torch.clone(text_hid_states[d_batch:])[config_back['id_uclip_emb']])\n",
        "                unclip_embs_2rt.append(torch.clone(base_unclip_embs[d_batch:])[config_back['id_uclip_emb']])\n",
        "                base_img_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_emb_s']])\n",
        "                image_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_delta']])\n",
        "                delta_2rt.append(torch.tensor(config_back['delta']).unsqueeze(1))\n",
        "                direction_2rt.append(torch.ones_like(delta_2rt[-1]))\n",
        "\n",
        "            # concat tansors\n",
        "            if rote_train > B_batch:\n",
        "              R_batch = B_batch\n",
        "            else: R_batch = rote_train\n",
        "\n",
        "            # shufle and take R_batch samples\n",
        "            text_hid_states_2rt = torch.concat(text_hid_states_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "            unclip_embs_2rt = torch.concat(unclip_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "            base_img_embs_2rt = torch.concat(base_img_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "            image_embs_2rt = torch.concat(image_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "            delta_2rt = torch.concat(delta_2rt)[s_idx][s_idx][:R_batch].to(DEVICE)\n",
        "            direction_2rt = torch.concat(direction_2rt)[s_idx][:R_batch].to(DEVICE)\n",
        "\n",
        "            # get rotation vectors\n",
        "            R_marixes = RV.get_rotation_matrix(base_img_embs_2rt.squeeze(1), image_embs_2rt.squeeze(1)).to(DEVICE)\n",
        "            text_hid_states_2rt = torch.bmm(text_hid_states_2rt, R_marixes)\n",
        "            unclip_embs_2rt = torch.bmm(unclip_embs_2rt, R_marixes)\n",
        "\n",
        "            # rotation predict\n",
        "            pred_rote_embs = model(\n",
        "                          text_hidden_states = text_hid_states_2rt.to(torch.float32),\n",
        "                          prior_embeds = unclip_embs_2rt.to(torch.float32),\n",
        "                          delta_time = delta_2rt,\n",
        "                          direction = direction_2rt\n",
        "                                            )\n",
        "\n",
        "            # difference\n",
        "            diff_unclip_embeds = (unclip_embs_2rt.squeeze(dim=1) - pred_rote_embs.squeeze(dim=1))\n",
        "            diff_img_embs =  (base_img_embs_2rt.squeeze(dim=1) - image_embs_2rt.squeeze(dim=1))\n",
        "\n",
        "            # CosineEmbeddingLoss between difference\n",
        "            movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  #\n",
        "\n",
        "            # MSELoss between predict and\n",
        "            movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n",
        "\n",
        "            movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n",
        "\n",
        "            del(delta_2rt, direction_2rt, diff_img_embs, diff_unclip_embeds)\n",
        "            del(pred_rote_embs, text_hid_states_2rt, R_marixes)\n",
        "            del(unclip_embs_2rt,image_embs_2rt)\n",
        "\n",
        "        # Diff train steps\n",
        "        diff_norm = len(config_diff_norm['id_uclip_emb'])\n",
        "        diff_back = len(config_diff_back['id_uclip_emb'])\n",
        "        diff_train = diff_norm + diff_back\n",
        "\n",
        "        if diff_train:\n",
        "            take_text_hid_states = []\n",
        "            take_base_unclip_embs = []\n",
        "            next_unclip_embs = []\n",
        "            next_base_img_embs = []\n",
        "            next_image_embs = []\n",
        "            next_delta = []\n",
        "            next_text_hid_states = []\n",
        "            next_direction = []\n",
        "\n",
        "            # un_shuffleed\n",
        "            pred_unclip_embs = torch.clone(pred_unclip_embs.detach().cpu())[bu_idx]\n",
        "\n",
        "            # intit class for shufflee again\n",
        "            srs = Shuff_Reshuff(diff_train)\n",
        "            s_idx = srs.shuffle() # shuffleed indexees\n",
        "            u_idx = srs.unshuffle() # indexees for re_shuffle\n",
        "\n",
        "            if diff_norm:\n",
        "                # collect next norm steps\n",
        "                take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n",
        "                take_text_hid_states.append(torch.clone(text_hid_states[:d_batch])[config_diff_norm['id_uclip_emb']])\n",
        "                next_unclip_embs.append(torch.clone(pred_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n",
        "                next_base_img_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_emb_s']])\n",
        "                next_image_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_delta']])\n",
        "                next_delta.append(torch.tensor(config_diff_norm['delta']).unsqueeze(1))\n",
        "                next_direction.append(torch.zeros_like(next_delta[-1]))\n",
        "\n",
        "            if diff_back:\n",
        "                # collect next back steps\n",
        "                take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_back['id_uclip_emb']])\n",
        "                take_text_hid_states.append(torch.clone(text_hid_states[d_batch:])[config_diff_back['id_uclip_emb']])\n",
        "                next_unclip_embs.append(torch.clone(pred_unclip_embs[d_batch:])[config_diff_back['id_uclip_emb']])\n",
        "                next_base_img_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_emb_s']])\n",
        "                next_image_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_delta']])\n",
        "                next_delta.append(torch.tensor(config_diff_back['delta']).unsqueeze(1))\n",
        "                next_direction.append(torch.ones_like(next_delta[-1]))\n",
        "\n",
        "\n",
        "\n",
        "            take_base_unclip_embs = torch.concat(take_base_unclip_embs).to(DEVICE)\n",
        "            take_text_hid_states = torch.concat(take_text_hid_states).to(DEVICE)\n",
        "            next_unclip_embs = torch.concat(next_unclip_embs).to(DEVICE)\n",
        "            next_base_img_embs = torch.concat(next_base_img_embs)\n",
        "            next_image_embs = torch.concat(next_image_embs)\n",
        "            next_delta = torch.concat(next_delta)[s_idx].to(DEVICE)\n",
        "            next_direction = torch.concat(next_direction)[s_idx].to(DEVICE)\n",
        "\n",
        "            # get rotation vectors\n",
        "            R_marixes = RV.get_rotation_matrix(take_base_unclip_embs.squeeze(1), next_unclip_embs.squeeze(1))\n",
        "            next_text_hid_states = torch.bmm(take_text_hid_states, R_marixes)\n",
        "\n",
        "            # dif predict from base predict\n",
        "            next_pred_unclip_embs = model(\n",
        "                text_hidden_states = next_text_hid_states[s_idx].to(torch.float32),\n",
        "                prior_embeds = next_unclip_embs[s_idx].to(torch.float32),\n",
        "                delta_time = next_delta,\n",
        "                direction = next_direction\n",
        "                )\n",
        "\n",
        "            # difference\n",
        "            diff_unclip_embeds = (next_unclip_embs[s_idx].squeeze(dim=1).to(DEVICE) - next_pred_unclip_embs.squeeze(dim=1))\n",
        "            diff_img_embs =  (next_base_img_embs[s_idx].squeeze(dim=1) - next_image_embs[s_idx].squeeze(dim=1))\n",
        "\n",
        "            # CosineEmbeddingLoss between difference\n",
        "            movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n",
        "\n",
        "            # MSELoss between predict and\n",
        "            movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n",
        "\n",
        "            # collect loss\n",
        "            movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n",
        "\n",
        "            del(diff_img_embs, diff_unclip_embeds)\n",
        "            del(next_image_embs, next_base_img_embs, next_delta, next_direction)\n",
        "            del(next_text_hid_states,  next_unclip_embs)\n",
        "            del(pred_unclip_embs, next_pred_unclip_embs)\n",
        "\n",
        "\n",
        "    movi_loss.backward(torch.ones_like(movi_loss))\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    #flush_memory()\n",
        "    cur_lr = scheduler.get_last_lr()[0]\n",
        "    cur_lr = round(cur_lr, 5)\n",
        "\n",
        "    print(f'ep {ep} movi_loss {movi_loss.mean().item()/len(all_data)} with lr {cur_lr}', end ='\\r')\n",
        "    print()\n",
        "    time.sleep(1)\n",
        "\n",
        "    # https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n",
        "    LR_RATE*=0.97\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
        "                                                  cycle_momentum = False,\n",
        "                                                  base_lr=LR_RATE/10,\n",
        "                                                  max_lr=LR_RATE,\n",
        "                                                  step_size_up=5,\n",
        "                                                  mode=\"triangular2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "3ddf5ae9168b4850b570e967afea554d",
            "0fb72a837d7042c89ac41355267b6ea9",
            "df9c5898436247aca2e019e32ef4f4bf",
            "188f9af29ffc4cb0a885ef5ec17bbe74",
            "1a83df3cd5694eb6a42728c99cf4bcb5",
            "ffe2256088504ae8bab43f4e4b08d759",
            "492e468e5ff54d1da0313241ed4a1441",
            "0f1e0dfebb3746fcb1750cfed0aa6677",
            "88e1945e69dc4acdaa163e4790c92f4e",
            "26c8880bd27c4c76892928f715c9af40",
            "d411f55c675940a7960eec318dbc88fd"
          ]
        },
        "id": "khfIn0ACOIBX",
        "outputId": "fdf9acaa-e23f-4111-ed51-d2a2a5038390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Пробегаемся по всем эпохам:   0%|          | 0/10 [00:00<?, ?EPOHS/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ddf5ae9168b4850b570e967afea554d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 movi_loss 0.05015625 with lr 0.00028\n",
            "ep 1 movi_loss 0.0512890625 with lr 0.00027\n",
            "ep 2 movi_loss 0.0525 with lr 0.00026\n",
            "ep 3 movi_loss 0.05078125 with lr 0.00026\n",
            "ep 4 movi_loss 0.049921875 with lr 0.00025\n",
            "ep 5 movi_loss 0.0497265625 with lr 0.00024\n",
            "ep 6 movi_loss 0.050703125 with lr 0.00023\n",
            "ep 7 movi_loss 0.050390625 with lr 0.00023\n",
            "ep 8 movi_loss 0.05125 with lr 0.00022\n",
            "ep 9 movi_loss 0.0522265625 with lr 0.00021\n"
          ]
        }
      ]
    }
  ]
}