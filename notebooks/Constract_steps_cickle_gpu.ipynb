{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Mike030668/Spliter_Text2Movi_Kandinskiy_22/blob/main/notebooks/Constract_steps_cickle_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1150,"status":"ok","timestamp":1710264589819,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"2dCFBzIDVjLQ"},"outputs":[],"source":["!git clone https://github.com/Mike030668/Spliter_Text2Movi_Kandinskiy_22.git -q"]},{"cell_type":"markdown","metadata":{"id":"TJNOkxoJJOG_"},"source":["# Construct method to find points of start and end fragments movi"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1710264625870,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"WRSn86u98HGl","outputId":"4ca50da5-ad7c-4f81-e060-28f4d059a7db"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import numpy as np\n","import torch\n","from torch import nn\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","MAIN_DIR =  \"/content/Spliter_Text2Movi_Kandinskiy_22\"\n","import sys\n","sys.path.append(MAIN_DIR)\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"zB0Kzp-DakPO"},"source":["## init rand data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1710264628010,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"JQ8htmqkWz0c"},"outputs":[],"source":["from utills.step_points import ComputeDiffPoints, Shuff_Reshuff"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1710236373399,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"_3aRPe-3Y5Ay","outputId":"261c3d60-2bda-4a3d-ba99-d6294cdb2e5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 0 14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","Time labels [ 0 14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","\n","\n","Norm_way\n","Start base point 0\n","Base normal points [14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","\n","\n","\n","Back_way\n","Start base back_point 0\n","Base back points [ 9 36 96 76 57 75 86 19  2 17 64 94 79 85 99]\n","\n","\n"]}],"source":["cdp = ComputeDiffPoints()\n","\n","# make set rand embeddings for tests\n","d_batch = 15\n","MAX_LEN_MOVI = 100\n","\n","# sets images embeds\n","image_embeds = torch.randn(d_batch+1, 1, 1280)\n","\n","# text embedings prompt video from Text model\n","text_hid_state = torch.randn((1, 77, 1280), requires_grad=True)\n","\n","# unclip embedings prompt video from Prior\n","unclip_embed = torch.randn((1, 1, 1280), requires_grad=True)\n","\n","# make set time_labels\n","labels = np.random.choice(np.arange(1, MAX_LEN_MOVI), d_batch+1, replace=False)\n","\n","# start and end points to use for start normal and back ways\n","labels[0] = 0\n","labels[-1] = MAX_LEN_MOVI-1\n","\n","print(labels)\n","cdp.time_labels = labels\n","config_norm, config_back = cdp.getpoints_train()\n","\n","print(f\"Time labels {labels}\\n\")\n","print('\\nNorm_way')\n","print(f\"Start base point {cdp.s_point}\")\n","print(f\"Base normal points {cdp.points}\\n\\n\")\n","print('\\nBack_way')\n","print(f\"Start base back_point {cdp.back_s_point}\")\n","print(f\"Base back points {cdp.back_points}\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"-Oq9QFVcaqLb"},"source":["## compute normal and back points for rotation steps"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1710236373400,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"_Qp3aR2hurQB","outputId":"0e916a73-6d96-4015-97c5-c9d4c77446d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time labels [ 0 14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","\n","\n","Norm rotation way\n","Base normal points [14 20  5 35 82 97 80 13 24 42 23  3 63 90 99] for start rotation steps \n","\n","ID image and text start rotation points [2, 7, 11, 0, 2, 7, 11, 11, 0, 1, 2, 7, 8, 10, 11, 0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 2, 11, 0, 1, 2, 7, 10, 11, 0, 1, 2, 3, 7, 8, 10, 11, 0, 1, 2, 7, 11, 0, 1, 2, 3, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","ID uclip_emb start rotation points [2, 7, 11, 0, 2, 7, 11, 11, 0, 1, 2, 7, 8, 10, 11, 0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 2, 11, 0, 1, 2, 7, 10, 11, 0, 1, 2, 3, 7, 8, 10, 11, 0, 1, 2, 7, 11, 0, 1, 2, 3, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","ID delta rotation points [0, 0, 0, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n","Delta between points [9, 1, 11, 6, 15, 7, 17, 2, 21, 15, 30, 22, 11, 12, 32, 68, 62, 77, 47, 2, 69, 58, 40, 59, 79, 19, 83, 77, 92, 62, 15, 17, 84, 73, 55, 74, 94, 34, 7, 66, 60, 75, 45, 67, 56, 38, 57, 77, 17, 8, 10, 10, 4, 19, 11, 1, 21, 28, 22, 37, 7, 29, 18, 19, 39, 9, 3, 18, 10, 20, 49, 43, 58, 28, 50, 39, 21, 40, 60, 76, 70, 85, 55, 8, 10, 77, 66, 48, 67, 87, 27, 85, 79, 94, 64, 17, 2, 19, 86, 75, 57, 76, 96, 36, 9] normal points\n","\n","\n","Norm rotation way\n","Base back points [ 9 36 96 76 57 75 86 19  2 17 64 94 79 85 99] for start rotation back steps \n","\n","ID image and text start rotation points [8, 0, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 4, 5, 7, 8, 9, 10, 0, 1, 7, 8, 9, 0, 1, 4, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 13, 0, 8, 9, 0, 8, 0, 1, 4, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 0, 1, 3, 4, 5, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] in base back points\n","ID uclip_emb start rotation points [8, 0, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 4, 5, 7, 8, 9, 10, 0, 1, 7, 8, 9, 0, 1, 4, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 13, 0, 8, 9, 0, 8, 0, 1, 4, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 0, 1, 3, 4, 5, 7, 8, 9, 10, 0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] in base back points\n","ID delta rotation points [0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14] in base back points\n","Delta between points [7, 27, 17, 34, 19, 87, 60, 20, 39, 21, 10, 77, 94, 79, 32, 2, 17, 11, 67, 40, 19, 1, 57, 74, 59, 12, 48, 21, 38, 55, 40, 66, 39, 18, 56, 73, 58, 11, 77, 50, 10, 29, 11, 67, 84, 69, 22, 7, 1, 10, 17, 2, 8, 15, 55, 28, 7, 45, 62, 47, 85, 58, 18, 37, 19, 8, 75, 92, 77, 30, 15, 9, 70, 43, 3, 22, 4, 60, 77, 62, 15, 76, 49, 9, 28, 10, 66, 83, 68, 21, 6, 90, 63, 3, 23, 42, 24, 13, 80, 97, 82, 35, 5, 20, 14] back points\n"]}],"source":["print(f\"Time labels {labels}\")\n","print('\\n\\nNorm rotation way')\n","print(f\"Base normal points {cdp.points} for start rotation steps \\n\")\n","id_img_emb_s = config_norm[\"id_img_emb_s\"]\n","print(f\"ID image and text start rotation points {id_img_emb_s}\")\n","id_uclip_emb = config_norm[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start rotation points {id_uclip_emb}\")\n","id_img_delta = config_norm[\"id_img_delta\"]\n","print(f\"ID delta rotation points {id_img_delta}\")\n","delta = config_norm[\"delta\"]\n","print(f\"Delta between points {delta} normal points\")\n","\n","\n","print('\\n\\nNorm rotation way')\n","print(f\"Base back points {cdp.back_points} for start rotation back steps \\n\")\n","id_img_emb_s = config_back[\"id_img_emb_s\"]\n","print(f\"ID image and text start rotation points {id_img_emb_s} in base back points\")\n","id_uclip_emb = config_back[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start rotation points {id_uclip_emb} in base back points\")\n","id_img_delta = config_back[\"id_img_delta\"]\n","print(f\"ID delta rotation points {id_img_delta} in base back points\")\n","delta = config_back[\"delta\"]\n","print(f\"Delta between points {delta} back points\")\n"]},{"cell_type":"markdown","metadata":{"id":"Tp7w3zw7TmDp"},"source":["## compute normal and back points for diff steps\n","\n","It will be used for prediction from prediction of model"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1710236373400,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"D2Q7stqHZbbM","outputId":"b906742b-dd15-4020-da4f-6edbf898e1b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time labels [ 0 14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","\n","\n","Norm diff way\n","Base normal points [14 20  5 35 82 97 80 13 24 42 23  3 63 90 99]\n","Next normal points [ 15  21   6  36  83  98  81  14  25  43  24   4  64  91 100] for diff steps \n","\n","ID image and text start diff points [0] in base normal points\n","ID uclip_emb start diff points [7] in next normal points\n","ID delta diff points [8] in next normal points\n","Delta between diff points [10] in base normal points\n","\n","\n","Norm rotation way\n","Base back points [ 9 36 96 76 57 75 86 19  2 17 64 94 79 85 99]\n","Next back points [ 10  37  97  77  58  76  87  20   3  18  65  95  80  86 100] for diff steps \n","\n","ID image and text start diff points [3] in base back points\n","ID uclip_emb start diff points [5] in next back points\n","ID delta diff points [6] in next back points\n","Delta between diff points [10] back points\n"]}],"source":["config_diff_norm, config_diff_back = cdp.getpoints_diftrain()\n","\n","print(f\"Time labels {labels}\")\n","print('\\n\\nNorm diff way')\n","print(f\"Base normal points {cdp.points}\")\n","print(f\"Next normal points {cdp.next_points} for diff steps \\n\")\n","id_img_emb_s = config_diff_norm[\"id_img_emb_s\"]\n","print(f\"ID image and text start diff points {id_img_emb_s} in base normal points\")\n","id_uclip_emb = config_diff_norm[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start diff points {id_uclip_emb} in next normal points\")\n","id_img_delta = config_diff_norm[\"id_img_delta\"]\n","print(f\"ID delta diff points {id_img_delta} in next normal points\")\n","delta = config_diff_norm[\"delta\"]\n","print(f\"Delta between diff points {delta} in base normal points\")\n","\n","\n","print('\\n\\nNorm rotation way')\n","print(f\"Base back points {cdp.back_points}\")\n","print(f\"Next back points {cdp.back_next_points} for diff steps \\n\")\n","id_img_emb_s = config_diff_back[\"id_img_emb_s\"]\n","print(f\"ID image and text start diff points {id_img_emb_s} in base back points\")\n","id_uclip_emb = config_diff_back[\"id_uclip_emb\"]\n","print(f\"ID uclip_emb start diff points {id_uclip_emb} in next back points\")\n","id_img_delta = config_diff_back[\"id_img_delta\"]\n","print(f\"ID delta diff points {id_img_delta} in next back points\")\n","delta = config_diff_back[\"delta\"]\n","print(f\"Delta between diff points {delta} back points\")\n"]},{"cell_type":"markdown","metadata":{"id":"eRPXhwREGqQI"},"source":["# Both ways steps model"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710264645886,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"EH5Kf3UVY3gF"},"outputs":[],"source":["from models.R_baseline import R_SpliterSimple\n","from utills.clearing import flush_memory\n","\n","model = R_SpliterSimple(max_delta_time = 100,\n","                        emb_dim = 1280,\n","                        ways = 2\n","                        )"]},{"cell_type":"markdown","metadata":{"id":"fLGgqgDL778b"},"source":["## Losses\n","\n","nn.CosineEmbeddingLoss\n","https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":749,"status":"ok","timestamp":1710236374142,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"l05KihJir06z","outputId":"df463eed-6ec5-4a19-f0aa-3c0ff3c0f3e6"},"outputs":[{"data":{"text/plain":["torch.Size([2, 1, 1280])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["clip_text_last_hidden_state = torch.randn((2, 77, 1280)) #outputs.last_hidden_state\n","embeddings = torch.randn((2, 1, 1280))\n","delta = torch.tensor([[3],[5]])\n","direction  = torch.tensor([[0],[1]])\n","model.to(DEVICE)\n","out_embeddings = model(text_hidden_states = clip_text_last_hidden_state.to(DEVICE),\n","                       prior_embeds = embeddings.to(DEVICE),\n","                       delta_time = delta.to(DEVICE),\n","                       direction = direction.to(DEVICE)\n","                        )\n","out_embeddings.shape"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1710236374142,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"YRthV5Nc5bHJ","outputId":"ade0b8e1-cf07-44a8-ed32-e306713676a4"},"outputs":[{"data":{"text/plain":["tensor(0.1178, grad_fn=<MeanBackward0>)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\n","loss = nn.CosineEmbeddingLoss()\n","input1 = torch.randn(3, 1, requires_grad=True)\n","input2 = torch.randn(3, 5, requires_grad=True)\n","target = torch.ones(3)\n","output = loss(input1, input2, target)\n","output.backward()\n","output"]},{"cell_type":"markdown","metadata":{"id":"GVtldMSOUWx7"},"source":["## apply as vector losses"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1710236374815,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"2g-uuGIc6Zm_","outputId":"7013f1d1-ea2e-48c6-dfbf-e7d7c21405a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0189, 1.3426, 0.5812,  ..., 0.9651, 1.4162, 1.0266], device='cuda:0',\n","       grad_fn=<AddBackward0>)\n"]}],"source":["mse_loss = nn.MSELoss(reduction='none').to(DEVICE)\n","cos_loss = nn.CosineEmbeddingLoss(reduction = 'none').to(DEVICE)\n","target = torch.ones(1280).to(DEVICE)\n","loss_cos = cos_loss(embeddings.squeeze(1).T.to(DEVICE), out_embeddings.squeeze(1).T, target)\n","print(loss_cos)"]},{"cell_type":"markdown","metadata":{"id":"cL-QqPMYGqQI"},"source":["## first steps normal and back shuffleed"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1710236374815,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"mO_PB6bQGqQJ","outputId":"3d832881-4eb7-455f-f3c1-035b60068d11"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2.1152, 2.0859, 1.7051,  ..., 2.8652, 1.6582, 5.3086], device='cuda:0',\n","       dtype=torch.float16, grad_fn=<AddBackward0>)\n"]}],"source":["all_points = np.append(cdp.points, cdp.back_points)\n","# intit class for shufflee\n","b_srs = Shuff_Reshuff(len(all_points))\n","bs_idx = b_srs.shuffle() # shuffleed indexees\n","bu_idx = b_srs.unshuffle() # indexees for re_shuffle\n","\n","B_batch = 2*d_batch\n","\n","time_base = torch.tensor(all_points).unsqueeze(1)\n","directions = torch.concat([torch.zeros_like(time_base[:d_batch]), torch.ones_like(time_base[d_batch:])])\n","\n","# shuffleed\n","time_base = time_base.to(DEVICE)\n","directions = directions.to(DEVICE)\n","\n","\n","text_hid_states =  torch.concat([text_hid_state for _ in range(B_batch)])\n","base_unclip_embs = torch.concat([unclip_embed for _ in range(B_batch)])\n","\n","base_img_embs = [image_embeds[0] for _ in range(d_batch)] + [image_embeds[-1] for _ in range(d_batch)]\n","base_img_embs = torch.concat(base_img_embs)\n","\n","\n","back_img_embs = torch.concat([image_embeds[i,:,:] for i in range(image_embeds.shape[0])][::-1]).unsqueeze(1)\n","img_embs =[image_embeds[1:,:,:], back_img_embs[1:,:,:]]\n","img_embs = torch.concat(img_embs)\n","\n","\n","pred_unclip_embs = model(\n","      text_hidden_states = text_hid_states[bs_idx].to(torch.float32).to(DEVICE), # shuffleed\n","      prior_embeds = base_unclip_embs[bs_idx].to(torch.float32).to(DEVICE),\n","      delta_time = time_base[bs_idx],\n","      direction = directions[bs_idx]\n","                        )\n","\n","# difference\n","diff_img_embs =  (base_img_embs[bs_idx].squeeze(dim=1) - img_embs[bs_idx].squeeze(dim=1)) #\n","diff_unclip_embs = (base_unclip_embs[bs_idx].squeeze(dim=1).to(DEVICE) - pred_unclip_embs.squeeze(dim=1))\n","\n","# CosineEmbeddingLoss between difference\n","movi_cos_loss = cos_loss(diff_unclip_embs.T, diff_img_embs.T.to(DEVICE), target).half()  #\n","\n","# MSELoss between predict and\n","movi_mse_loss = mse_loss(diff_unclip_embs.half(), diff_img_embs.half().to(DEVICE))#\n","\n","movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","del(time_base, directions, diff_img_embs, diff_unclip_embs)\n","flush_memory()\n","\n","print(movi_loss)"]},{"cell_type":"markdown","metadata":{"id":"zWh3qG0Nseff"},"source":["## Rotation steps normal and back shuffleed"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1710236374816,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"IF4rY1xAwInw"},"outputs":[],"source":["from utills.rotations import RotationVectors\n","# inite rotation class\n","RV = RotationVectors()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2450,"status":"ok","timestamp":1710236377261,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"3uY7utEtutNJ","outputId":"4acef2cd-d8bf-4933-c8ae-933e5d388d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([6.2969, 4.8281, 5.9922,  ..., 7.5547, 6.8047, 8.6719], device='cuda:0',\n","       dtype=torch.float16, grad_fn=<AddBackward0>)\n"]}],"source":["rote_norm_train = len(config_norm['id_uclip_emb'])\n","rote_back_train = len(config_back['id_uclip_emb'])\n","rote_train =  rote_norm_train + rote_back_train\n","\n","if rote_train:\n","    text_hid_states_2rt = []\n","    unclip_embs_2rt = []\n","    base_img_embs_2rt = []\n","    image_embs_2rt = []\n","    delta_2rt = []\n","    direction_2rt = []\n","\n","    # intit class for shufflee again\n","    srs = Shuff_Reshuff(rote_train)\n","    s_idx = srs.shuffle() # shuffleed indexees\n","    u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","\n","    if rote_norm_train:\n","        # collect norm steps\n","        text_hid_states_2rt.append(torch.clone(text_hid_states[:d_batch])[config_norm['id_uclip_emb']])\n","        unclip_embs_2rt.append(torch.clone(base_unclip_embs[:d_batch])[config_norm['id_uclip_emb']])\n","        base_img_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_emb_s']])\n","        image_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_delta']])\n","        delta_2rt.append(torch.tensor(config_norm['delta']).unsqueeze(1))\n","        direction_2rt.append(torch.zeros_like(delta_2rt[-1]))\n","\n","\n","    if rote_back_train:\n","        # collect back steps\n","        text_hid_states_2rt.append(torch.clone(text_hid_states[d_batch:])[config_back['id_uclip_emb']])\n","        unclip_embs_2rt.append(torch.clone(base_unclip_embs[d_batch:])[config_back['id_uclip_emb']])\n","        base_img_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_emb_s']])\n","        image_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_delta']])\n","        delta_2rt.append(torch.tensor(config_back['delta']).unsqueeze(1))\n","        direction_2rt.append(torch.ones_like(delta_2rt[-1]))\n","\n","    # concat tansors\n","    if rote_train > B_batch:\n","       R_batch = B_batch\n","    else: R_batch = rote_train\n","\n","    # shufle and take R_batch samples\n","    text_hid_states_2rt = torch.concat(text_hid_states_2rt)[s_idx][:R_batch].to(DEVICE)\n","    unclip_embs_2rt = torch.concat(unclip_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","    base_img_embs_2rt = torch.concat(base_img_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","    image_embs_2rt = torch.concat(image_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","    delta_2rt = torch.concat(delta_2rt)[s_idx][s_idx][:R_batch].to(DEVICE)\n","    direction_2rt = torch.concat(direction_2rt)[s_idx][:R_batch].to(DEVICE)\n","\n","    # get rotation vectors\n","    R_marixes = RV.get_rotation_matrix(base_img_embs_2rt.squeeze(1), image_embs_2rt.squeeze(1)).to(DEVICE)\n","    text_hid_states_2rt = torch.bmm(text_hid_states_2rt, R_marixes)\n","    unclip_embs_2rt = torch.bmm(unclip_embs_2rt, R_marixes)\n","\n","    pred_rote_embs = model(\n","                  text_hidden_states = text_hid_states_2rt.to(torch.float32), # shuffleed\n","                  prior_embeds = unclip_embs_2rt.to(torch.float32),\n","                  delta_time = delta_2rt,\n","                  direction = direction_2rt\n","                                    )\n","\n","    # difference\n","    diff_unclip_embeds = (unclip_embs_2rt.squeeze(dim=1) - pred_rote_embs.squeeze(dim=1))\n","    diff_img_embs =  (base_img_embs_2rt.squeeze(dim=1) - image_embs_2rt.squeeze(dim=1))\n","\n","    # CosineEmbeddingLoss between difference\n","    movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n","\n","    # MSELoss between predict and\n","    movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n","\n","    movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","    del(delta_2rt, direction_2rt, diff_img_embs, diff_unclip_embeds)\n","    del(pred_rote_embs, text_hid_states_2rt, R_marixes)\n","    del(unclip_embs_2rt,image_embs_2rt)\n","    flush_memory()\n","\n","\n","    print(movi_loss)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FEz3tuuKBMHD"},"source":["## Next steps normal and back from pred steps shuffleed"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":806,"status":"ok","timestamp":1710236378064,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"zQg68d4NGqQJ","outputId":"e7715b15-f0a7-4abf-9475-b43be13d14ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.0312, 5.4102, 0.1377,  ..., 0.1731, 4.2227, 4.5703], device='cuda:0',\n","       dtype=torch.float16, grad_fn=<AddBackward0>)\n"]}],"source":["diff_norm = len(config_diff_norm['id_uclip_emb'])\n","diff_back = len(config_diff_back['id_uclip_emb'])\n","diff_train = diff_norm + diff_back\n","\n","if diff_train:\n","    take_text_hid_states = []\n","    take_base_unclip_embs = []\n","    next_unclip_embs = []\n","    next_base_img_embs = []\n","    next_image_embs = []\n","    next_delta = []\n","    next_text_hid_states = []\n","    next_direction = []\n","\n","    # un_shuffleed\n","    pred_unclip_embs = torch.clone(pred_unclip_embs.detach().cpu())[bu_idx]\n","\n","    # intit class for shufflee again\n","    srs = Shuff_Reshuff(diff_train)\n","    s_idx = srs.shuffle() # shuffleed indexees\n","    u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","    if diff_norm:\n","        # collect next norm steps\n","        take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","        take_text_hid_states.append(torch.clone(text_hid_states[:d_batch])[config_diff_norm['id_uclip_emb']])\n","        next_unclip_embs.append(torch.clone(pred_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","        next_base_img_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_emb_s']])\n","        next_image_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_delta']])\n","        next_delta.append(torch.tensor(config_diff_norm['delta']).unsqueeze(1))\n","        next_direction.append(torch.zeros_like(next_delta[-1]))\n","\n","    if diff_back:\n","        # collect next back steps\n","        take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_back['id_uclip_emb']])\n","        take_text_hid_states.append(torch.clone(text_hid_states[d_batch:])[config_diff_back['id_uclip_emb']])\n","        next_unclip_embs.append(torch.clone(pred_unclip_embs[d_batch:])[config_diff_back['id_uclip_emb']])\n","        next_base_img_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_emb_s']])\n","        next_image_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_delta']])\n","        next_delta.append(torch.tensor(config_diff_back['delta']).unsqueeze(1))\n","        next_direction.append(torch.ones_like(next_delta[-1]))\n","\n","\n","\n","    take_base_unclip_embs = torch.concat(take_base_unclip_embs).to(DEVICE)\n","    take_text_hid_states = torch.concat(take_text_hid_states).to(DEVICE)\n","    next_unclip_embs = torch.concat(next_unclip_embs).to(DEVICE)\n","    next_base_img_embs = torch.concat(next_base_img_embs)\n","    next_image_embs = torch.concat(next_image_embs)\n","    next_delta = torch.concat(next_delta)[s_idx].to(DEVICE)\n","    next_direction = torch.concat(next_direction)[s_idx].to(DEVICE)\n","\n","    # get rotation vectors\n","    R_marixes = RV.get_rotation_matrix(take_base_unclip_embs.squeeze(1), next_unclip_embs.squeeze(1))\n","    next_text_hid_states = torch.bmm(take_text_hid_states, R_marixes)\n","\n","\n","    next_pred_unclip_embs = model(\n","        text_hidden_states = next_text_hid_states[s_idx].to(torch.float32),\n","        prior_embeds = next_unclip_embs[s_idx].to(torch.float32),\n","        delta_time = next_delta,\n","        direction = next_direction\n","        )\n","\n","    # difference\n","    diff_unclip_embeds = (next_unclip_embs[s_idx].squeeze(dim=1).to(DEVICE) - next_pred_unclip_embs.squeeze(dim=1))\n","    diff_img_embs =  (next_base_img_embs[s_idx].squeeze(dim=1) - next_image_embs[s_idx].squeeze(dim=1))\n","\n","    # CosineEmbeddingLoss between difference\n","    movi_cos_loss = cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n","\n","    # MSELoss between predict and\n","    movi_mse_loss = mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)) #\n","\n","    next_movi_loss = movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","    del(diff_img_embs, diff_unclip_embeds)\n","    del(next_image_embs, next_base_img_embs, next_delta, next_direction)\n","    del(next_text_hid_states,  next_unclip_embs)\n","    del(pred_unclip_embs, next_pred_unclip_embs)\n","    flush_memory()\n","\n","    print(next_movi_loss)"]},{"cell_type":"markdown","metadata":{"id":"XW6o9w1mOQfS"},"source":["##  Cicle"]},{"cell_type":"markdown","metadata":{"id":"u2c2tlUzNLRe"},"source":["### init rand data"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":325,"status":"ok","timestamp":1710264650900,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"j8ekLAdPOQfT"},"outputs":[],"source":["all_data = []\n","# make set rand embeddings for tests\n","d_batch = 15\n","q_ty = 100\n","\n","for i in range(q_ty):\n","    data = dict()\n","    # make set time_labels\n","    labels = np.random.choice(np.arange(1, 99), d_batch+1, replace=False)\n","    labels[0] = 0\n","    labels[-1] = 99\n","    data['labels'] = labels\n","\n","    data['image_embeds'] = []\n","    for i in range(d_batch+1):\n","      if not i : data['image_embeds'].append(torch.randn(1, 1, 1280))\n","      else:\n","        next = data['image_embeds'][-1] + 0.0001*torch.randn(1, 1, 1280)\n","        data['image_embeds'].append(next)\n","\n","    data['text_hid_state'] = torch.randn((1, 77, 1280))\n","    data['unclip_embed'] = torch.randn((1, 1, 1280), requires_grad=True)\n","\n","    all_data.append(data)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3491,"status":"ok","timestamp":1710264662065,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"5MrmU4BvOQfT"},"outputs":[],"source":["from utills.rotations import RotationVectors\n","# inite rotation class\n","LR_RATE = 1e-03\n","EPS = 0.95\n","\n","mse_loss = nn.MSELoss(reduction='none').to(DEVICE)\n","cos_loss = nn.CosineEmbeddingLoss(reduction = 'none').to(DEVICE)\n","target = torch.ones(1280).to(DEVICE)\n","\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR_RATE)\n","\n","# https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n","scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n","                                              cycle_momentum = False,\n","                                              base_lr=LR_RATE/10,\n","                                              max_lr=LR_RATE,\n","                                              step_size_up=5,\n","                                              mode=\"triangular2\")\n","\n","add_back_train = True\n","add_rote_train = True\n","add_diff_train = True\n","\n","model.to(DEVICE)\n","model.train()\n","\n","# inite rotation class\n","RV = RotationVectors()\n","\n","# inite ComputeDiffPoints class\n","cdp = ComputeDiffPoints(treshold = 0)"]},{"cell_type":"markdown","metadata":{"id":"ZWAmLlguOEtl"},"source":["### test train"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["b86075039d6745a3aa56bbc68c944d5f","668ac0f0e2184ef88db4c66afe0d5134","17ebd7fe59b144fd9e57a677b059bca1","d9562df49a214804b2f8285950637309","8948b961ebfe4a51ac85e8040b914a03","a063700538b748dda3c20964fa393738","9857403bc6574f499fa7ceb6e84e8617","d837cfb675a1436989d761f928233545","710379034f0c4067aadefeaf6649a3d5","3b649e2800be45d1ae2d5d1a758d95a0","607f17bdfb0d4f54bd6ec5659314eac3"]},"executionInfo":{"elapsed":50522,"status":"ok","timestamp":1710265223253,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"},"user_tz":-180},"id":"OU35XBiQ8Z69","outputId":"39c64364-b014-4448-cfc7-dd80f6d50c03"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b86075039d6745a3aa56bbc68c944d5f","version_major":2,"version_minor":0},"text/plain":["Пробегаемся по всем эпохам:   0%|          | 0/3 [00:00<?, ?EPOHS/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["diff_loss_weight 0\n","|\rEp 0 all_loss 0.02045 | acc 0.30065 | mse_loss 0.01049 | cos_loss 0.00997 | lr 0.00026\n","\n","diff_loss_weight 0\n","Ep 1 all_loss 0.02625 | acc 0.29902 | mse_loss 0.01314 | cos_loss 0.01310 | lr 0.00025\n","\n","diff_loss_weight 0.010506208213944768\n","Ep 2 all_loss 0.02566 | acc 0.29614 | mse_loss 0.01255 | cos_loss 0.01311 | lr 0.00024\n","\n"]}],"source":["# отрисовка прохождения цикла\n","from tqdm.notebook import tqdm\n","from random import shuffle\n","import time\n","\n","step = 0\n","EPOHS = 3\n","\n","hist = {\"loss\" : [],\n","        \"loss_cos\" : [],\n","        \"loss_mse\" : [],\n","        \"acc\" : [],\n","        \"base_loss\" : [],\n","        \"lr\" : []\n","}\n","\n","for epoch  in tqdm(range(EPOHS), unit =\"EPOHS\",\n","                      desc =\"Пробегаемся по всем эпохам\"):\n","    eph_loss = 0\n","    eph_loss_cos = 0\n","    eph_loss_mse = 0\n","    eph_cos_acc = 0\n","    eph_base_loss = 0\n","    cur_lr = optimizer.param_groups[0]['lr']\n","    diff_loss_weight = 0\n","\n","    shuffle(all_data)\n","    for movi in all_data:\n","\n","        optimizer.zero_grad()\n","        # make set rand embeddings for tests\n","        image_embeds = torch.concat(movi['image_embeds'])\n","        text_hid_state = movi['text_hid_state']\n","        unclip_embed = movi['unclip_embed']\n","        # make set time_labels\n","        labels = movi['labels']\n","\n","        # place labels to class points\n","        cdp.time_labels = labels\n","        all_points = cdp.points\n","\n","        if add_back_train:\n","            all_points = np.append(all_points, cdp.back_points)\n","\n","        # intit class for shufflee\n","        b_srs = Shuff_Reshuff(len(all_points))\n","        bs_idx = b_srs.shuffle() # shuffleed indexees\n","        bu_idx = b_srs.unshuffle() # indexees for re_shuffle\n","\n","        # collect time directions tensors\n","        time_base = torch.tensor(all_points).unsqueeze(1)\n","\n","        directions = [torch.zeros_like(time_base[:d_batch])]\n","        if add_back_train:\n","            directions.append(torch.ones_like(time_base[d_batch:]))\n","        directions = torch.concat(directions)\n","\n","\n","        # collect text tensors\n","        text_hid_states = [text_hid_state for _ in range(d_batch)]\n","        if add_back_train:\n","            text_hid_states.extend([text_hid_state for _ in range(d_batch)])\n","        text_hid_states =  torch.concat(text_hid_states)\n","\n","        # collect unclip tensors\n","        base_unclip_embs = [unclip_embed for _ in range(d_batch)]\n","        if add_back_train:\n","            base_unclip_embs.extend([unclip_embed for _ in range(d_batch)])\n","        base_unclip_embs = torch.concat(base_unclip_embs)\n","\n","        # collect base_img_embs tensors\n","        base_img_embs = [image_embeds[0] for _ in range(d_batch)]\n","        if add_back_train:\n","            base_img_embs.extend([image_embeds[-1] for _ in range(d_batch)])\n","        base_img_embs = torch.concat(base_img_embs)\n","\n","        # collect img_embs tensors\n","        img_embs = image_embeds[1:]\n","\n","        if add_back_train:\n","            # collect back_img_embs tensors\n","            back_img_embs = torch.flip(image_embeds, [0,])[1:]\n","            # collect img_embs together tensors\n","            img_embs = torch.concat([img_embs, back_img_embs])\n","\n","        # base predict which can used in next step\n","        pred_unclip_embs = model(\n","              text_hidden_states = text_hid_states[bs_idx].to(torch.float32).to(DEVICE), # shuffleed\n","              prior_embeds = base_unclip_embs[bs_idx].to(torch.float32).to(DEVICE),\n","              delta_time = time_base[bs_idx].to(DEVICE),\n","              direction = directions[bs_idx].to(DEVICE)\n","                                )\n","\n","        # difference\n","        diff_img_embs = (base_img_embs[bs_idx].squeeze(dim=1) - img_embs[bs_idx].squeeze(dim=1)) #\n","        diff_unclip_embs = (base_unclip_embs[bs_idx].squeeze(dim=1).to(DEVICE) - pred_unclip_embs.squeeze(dim=1))\n","\n","        # CosineEmbeddingLoss between difference\n","        movi_cos_loss = cos_loss(diff_unclip_embs.T, diff_img_embs.T.to(DEVICE), target).half()  #\n","\n","        # MSELoss between predict and\n","        movi_mse_loss = mse_loss(diff_unclip_embs.half(), diff_img_embs.half().to(DEVICE)).mean(0)#\n","\n","        movi_loss_base = movi_cos_loss + movi_mse_loss # (0) for 1280\n","        eph_base_loss = movi_loss_base.mean().item()\n","\n","        # CosineEmbeddingLoss between difference\n","        cos_acc = cos_loss(pred_unclip_embs.squeeze(1).T,\n","                           img_embs[bs_idx].squeeze(1).T.to(DEVICE), (-1)*target).half()\n","\n","        eph_cos_acc+= cos_acc.mean().item()\n","\n","        del(time_base, directions, diff_img_embs, diff_unclip_embs)\n","\n","        to_rote, rote_norm, rote_back = 0, 0, 0\n","        if add_rote_train:\n","            # Rotation train steps\n","            rote_norm = len(config_norm['id_uclip_emb'])\n","            rote_back = len(config_back['id_uclip_emb'])\n","            to_rote =  rote_norm + rote_back\n","\n","        if add_rote_train and epoch and to_rote:\n","            text_hid_states_2rt = []\n","            unclip_embs_2rt = []\n","            base_img_embs_2rt = []\n","            image_embs_2rt = []\n","            delta_2rt = []\n","            direction_2rt = []\n","\n","            # intit class for shufflee again\n","            srs = Shuff_Reshuff(to_rote)\n","            s_idx = srs.shuffle() # shuffleed indexees\n","            u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","\n","            if rote_norm:\n","                # collect norm steps\n","                text_hid_states_2rt.append(torch.clone(text_hid_states[:d_batch])[config_norm['id_uclip_emb']])\n","                unclip_embs_2rt.append(torch.clone(base_unclip_embs[:d_batch])[config_norm['id_uclip_emb']])\n","                base_img_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_emb_s']])\n","                image_embs_2rt.append(torch.clone(img_embs[:d_batch])[config_norm['id_img_delta']])\n","                delta_2rt.append(torch.tensor(config_norm['delta']).unsqueeze(1))\n","                direction_2rt.append(torch.zeros_like(delta_2rt[-1]))\n","\n","\n","            if rote_back and add_back_train:\n","                # collect back steps\n","                text_hid_states_2rt.append(torch.clone(text_hid_states[d_batch:])[config_back['id_uclip_emb']])\n","                unclip_embs_2rt.append(torch.clone(base_unclip_embs[d_batch:])[config_back['id_uclip_emb']])\n","                base_img_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_emb_s']])\n","                image_embs_2rt.append(torch.clone(img_embs[d_batch:])[config_back['id_img_delta']])\n","                delta_2rt.append(torch.tensor(config_back['delta']).unsqueeze(1))\n","                direction_2rt.append(torch.ones_like(delta_2rt[-1]))\n","\n","            # concat tansors\n","            if add_back_train:\n","              if len(direction_2rt) > 2*d_batch:\n","                R_batch = 2*d_batch\n","              else: R_batch = len(direction_2rt)\n","            else:\n","              if len(direction_2rt) > d_batch:\n","                R_batch = d_batch\n","              else: R_batch = len(direction_2rt)\n","\n","            # shufle and take R_batch samples\n","            text_hid_states_2rt = torch.concat(text_hid_states_2rt)[s_idx][:R_batch].to(DEVICE)\n","            unclip_embs_2rt = torch.concat(unclip_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","            base_img_embs_2rt = torch.concat(base_img_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","            image_embs_2rt = torch.concat(image_embs_2rt)[s_idx][:R_batch].to(DEVICE)\n","            delta_2rt = torch.concat(delta_2rt)[s_idx][s_idx][:R_batch].to(DEVICE)\n","            direction_2rt = torch.concat(direction_2rt)[s_idx][:R_batch].to(DEVICE)\n","\n","            # get rotation vectors\n","            R_marixes = RV.get_rotation_matrix(base_img_embs_2rt.squeeze(1),\n","                                               image_embs_2rt.squeeze(1)).to(DEVICE)\n","            text_hid_states_2rt = torch.bmm(text_hid_states_2rt, R_marixes)\n","            unclip_embs_2rt = torch.bmm(unclip_embs_2rt, R_marixes)\n","\n","            # rotation predict\n","            pred_rote_embs = model(\n","                          text_hidden_states = text_hid_states_2rt.to(torch.float32),\n","                          prior_embeds = unclip_embs_2rt.to(torch.float32),\n","                          delta_time = delta_2rt,\n","                          direction = direction_2rt\n","                                            )\n","\n","            # difference\n","            diff_unclip_embeds = (unclip_embs_2rt.squeeze(dim=1) - pred_rote_embs.squeeze(dim=1))\n","            diff_img_embs = (base_img_embs_2rt.squeeze(dim=1) - image_embs_2rt.squeeze(dim=1))\n","\n","            rote_loss_weight =hist[\"acc\"][-1]\n","            #print(\"rote_loss_weight\", rote_loss_weight)\n","            # CosineEmbeddingLoss between difference\n","            movi_cos_loss += rote_loss_weight*cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  #\n","\n","            # MSELoss between predict and\n","            movi_mse_loss += rote_loss_weight*mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)).mean(0) #\n","\n","            #movi_loss = movi_loss + movi_cos_loss + movi_mse_loss.mean(0) # (0) for 1280\n","\n","            del(delta_2rt, direction_2rt, diff_img_embs, diff_unclip_embeds)\n","            del(pred_rote_embs, text_hid_states_2rt, R_marixes)\n","            del(unclip_embs_2rt,image_embs_2rt)\n","\n","        # Diff train steps\n","        to_diff = 0\n","        if add_diff_train:\n","            config_diff_norm, config_diff_back = cdp.getpoints_diftrain()\n","            diff_norm = len(config_diff_norm['id_uclip_emb'])\n","            diff_back = len(config_diff_back['id_uclip_emb'])\n","            to_diff = diff_norm + diff_back\n","\n","        if add_diff_train and epoch>1 and to_diff:\n","            diff_loss_weight = 1 - hist[\"base_loss\"][-1]/hist[\"base_loss\"][0]\n","            if diff_loss_weight<0: diff_loss_weight = 0\n","\n","        if add_diff_train and epoch>1 and to_diff and diff_loss_weight:\n","            take_text_hid_states = []\n","            take_base_unclip_embs = []\n","            next_unclip_embs = []\n","            next_base_img_embs = []\n","            next_image_embs = []\n","            next_delta = []\n","            next_text_hid_states = []\n","            next_direction = []\n","\n","            # un_shuffleed\n","            pred_unclip_embs = torch.clone(pred_unclip_embs.detach().cpu())[bu_idx]\n","\n","            # intit class for shufflee again\n","            srs = Shuff_Reshuff(to_diff)\n","            s_idx = srs.shuffle() # shuffleed indexees\n","            u_idx = srs.unshuffle() # indexees for re_shuffle\n","\n","            if diff_norm:\n","                # collect next norm steps\n","                take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","                take_text_hid_states.append(torch.clone(text_hid_states[:d_batch])[config_diff_norm['id_uclip_emb']])\n","                next_unclip_embs.append(torch.clone(pred_unclip_embs[:d_batch])[config_diff_norm['id_uclip_emb']])\n","                next_base_img_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_emb_s']])\n","                next_image_embs.append(torch.clone(img_embs[:d_batch])[config_diff_norm['id_img_delta']])\n","                next_delta.append(torch.tensor(config_diff_norm['delta']).unsqueeze(1))\n","                next_direction.append(torch.zeros_like(next_delta[-1]))\n","\n","            if diff_back:\n","                # collect next back steps\n","                take_base_unclip_embs.append(torch.clone(base_unclip_embs[:d_batch])[config_diff_back['id_uclip_emb']])\n","                take_text_hid_states.append(torch.clone(text_hid_states[d_batch:])[config_diff_back['id_uclip_emb']])\n","                next_unclip_embs.append(torch.clone(pred_unclip_embs[d_batch:])[config_diff_back['id_uclip_emb']])\n","                next_base_img_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_emb_s']])\n","                next_image_embs.append(torch.clone(img_embs[d_batch:])[config_diff_back['id_img_delta']])\n","                next_delta.append(torch.tensor(config_diff_back['delta']).unsqueeze(1))\n","                next_direction.append(torch.ones_like(next_delta[-1]))\n","\n","\n","\n","            take_base_unclip_embs = torch.concat(take_base_unclip_embs).to(DEVICE)\n","            take_text_hid_states = torch.concat(take_text_hid_states).to(DEVICE)\n","            next_unclip_embs = torch.concat(next_unclip_embs).to(DEVICE)\n","            next_base_img_embs = torch.concat(next_base_img_embs)\n","            next_image_embs = torch.concat(next_image_embs)\n","            next_delta = torch.concat(next_delta)[s_idx].to(DEVICE)\n","            next_direction = torch.concat(next_direction)[s_idx].to(DEVICE)\n","\n","            # get rotation vectors\n","            R_marixes = RV.get_rotation_matrix(take_base_unclip_embs.squeeze(1), next_unclip_embs.squeeze(1))\n","            next_text_hid_states = torch.bmm(take_text_hid_states, R_marixes)\n","\n","            # dif predict from base predict\n","            next_pred_unclip_embs = model(\n","                text_hidden_states = next_text_hid_states[s_idx].to(torch.float32),\n","                prior_embeds = next_unclip_embs[s_idx].to(torch.float32),\n","                delta_time = next_delta,\n","                direction = next_direction\n","                )\n","\n","            # difference\n","            diff_unclip_embeds = (next_unclip_embs[s_idx].squeeze(dim=1).to(DEVICE) - next_pred_unclip_embs.squeeze(dim=1))\n","            diff_img_embs =  (next_base_img_embs[s_idx].squeeze(dim=1) - next_image_embs[s_idx].squeeze(dim=1))\n","\n","\n","            # CosineEmbeddingLoss between difference\n","            movi_cos_loss += diff_loss_weight * cos_loss(diff_unclip_embeds.T, diff_img_embs.T.to(DEVICE), target).half()  # .nan_to_num()\n","\n","            # MSELoss between predict and\n","            movi_mse_loss += diff_loss_weight * mse_loss(diff_unclip_embeds.half(), diff_img_embs.half().to(DEVICE)).mean(0) #\n","\n","            del(diff_img_embs, diff_unclip_embeds)\n","            del(next_image_embs, next_base_img_embs, next_delta, next_direction)\n","            del(next_text_hid_states,  next_unclip_embs)\n","            del(pred_unclip_embs, next_pred_unclip_embs)\n","\n","\n","    # collect loss\n","    movi_loss = movi_cos_loss + movi_mse_loss # (0) for 1280\n","    movi_loss.backward(torch.ones_like(movi_loss))\n","    eph_loss_mse += movi_mse_loss.mean().item()\n","    eph_loss_cos += movi_cos_loss.mean().item()\n","    eph_loss += movi_loss.mean().item()\n","\n","\n","    optimizer.step()\n","\n","    hist[\"lr\"].append(cur_lr)\n","    good_steps = len(all_data)\n","    eph_cos_acc/=good_steps\n","    eph_base_loss/=good_steps\n","    eph_loss/=good_steps\n","    eph_loss_mse/=good_steps\n","    eph_loss_cos/=good_steps\n","\n","    hist[\"loss\"].append(eph_loss)\n","    hist[\"loss_mse\"].append(eph_loss_mse)\n","    hist[\"loss_cos\"].append(eph_loss_cos)\n","    hist[\"acc\"].append(eph_cos_acc)\n","    hist[\"base_loss\"].append(eph_base_loss)\n","\n","    scheduler.step()\n","    flush_memory()\n","    cur_lr = scheduler.get_last_lr()[0]\n","    cur_lr = round(cur_lr, 5)\n","\n","    # https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n","    LR_RATE*=0.97\n","    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n","                                                  cycle_momentum = False,\n","                                                  base_lr=LR_RATE/10,\n","                                                  max_lr=LR_RATE,\n","                                                  step_size_up=5,\n","                                                  mode=\"triangular2\")\n","\n","    try: print(\"diff_loss_weight\", diff_loss_weight)\n","    except: pass\n","    print(f'\\rEp {epoch} all_loss {hist[\"loss\"][-1]:.5f} | acc {hist[\"acc\"][-1]:.5f} | mse_loss {hist[\"loss_mse\"][-1]:.5f} | cos_loss {hist[\"loss_cos\"][-1]:.5f} | lr {cur_lr:.5f}\\n')# {text}\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"17ebd7fe59b144fd9e57a677b059bca1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d837cfb675a1436989d761f928233545","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_710379034f0c4067aadefeaf6649a3d5","value":3}},"3b649e2800be45d1ae2d5d1a758d95a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"607f17bdfb0d4f54bd6ec5659314eac3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"668ac0f0e2184ef88db4c66afe0d5134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a063700538b748dda3c20964fa393738","placeholder":"​","style":"IPY_MODEL_9857403bc6574f499fa7ceb6e84e8617","value":"Пробегаемся по всем эпохам: 100%"}},"710379034f0c4067aadefeaf6649a3d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8948b961ebfe4a51ac85e8040b914a03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9857403bc6574f499fa7ceb6e84e8617":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a063700538b748dda3c20964fa393738":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86075039d6745a3aa56bbc68c944d5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_668ac0f0e2184ef88db4c66afe0d5134","IPY_MODEL_17ebd7fe59b144fd9e57a677b059bca1","IPY_MODEL_d9562df49a214804b2f8285950637309"],"layout":"IPY_MODEL_8948b961ebfe4a51ac85e8040b914a03"}},"d837cfb675a1436989d761f928233545":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9562df49a214804b2f8285950637309":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b649e2800be45d1ae2d5d1a758d95a0","placeholder":"​","style":"IPY_MODEL_607f17bdfb0d4f54bd6ec5659314eac3","value":" 3/3 [00:50&lt;00:00, 19.37s/EPOHS]"}}}}},"nbformat":4,"nbformat_minor":0}
